<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R21NVEB2EY"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-R21NVEB2EY');
    </script>
    
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cloud Infrastructure Technologies &mdash; tech.bitvijays.com 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=51b770b3"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/debug.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Open Source Concepts" href="LFF-ESS-P0E-OpenSource.html" />
    <link rel="prev" title="Linux Basics" href="LFF-ESS-P0B-LinuxEssentials.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            tech.bitvijays.com
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The Essentials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="LFF-ESS-P0A-CyberSecurityEnterprise.html">Cybersecurity in an Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="LFF-ESS-P0B-LinuxEssentials.html">Linux Basics</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Cloud Infrastructure Technologies</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#initial-cloud-concepts">Initial cloud concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cloud-computing">Cloud computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#virtualization">Virtualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hypervisor">Hypervisor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#type-1-native-or-bare-metal-hypervisors">Type-1, native or bare-metal hypervisors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#type-2-or-hosted-hypervisors">Type-2 or hosted hypervisors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#type-1-2-hypervisor-example-linux-kvm">Type-1/2 Hypervisor Example: Linux KVM</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#manage-kvm-vms">Manage KVM VMs</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#type-2-hypervisor-example-virtualbox">Type-2 Hypervisor Example: Virtualbox</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#vm-management">VM Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#vagrant">Vagrant</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#vagrant-file">Vagrant file</a></li>
<li class="toctree-l5"><a class="reference internal" href="#boxes">Boxes</a></li>
<li class="toctree-l5"><a class="reference internal" href="#providers">Providers</a></li>
<li class="toctree-l5"><a class="reference internal" href="#synced-folders">Synced Folders</a></li>
<li class="toctree-l5"><a class="reference internal" href="#provisioning">Provisioning</a></li>
<li class="toctree-l5"><a class="reference internal" href="#plugins">Plugins</a></li>
<li class="toctree-l5"><a class="reference internal" href="#networking">Networking</a></li>
<li class="toctree-l5"><a class="reference internal" href="#multi-machine">Multi-Machine</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deployment-models">Deployment Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#infrastructure-as-a-service">Infrastructure as a Service</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#iaas-providers">IaaS providers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#platform-as-a-service">Platform as a Service</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#paas-providers">PaaS providers</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#cloud-foundry-cf">Cloud Foundry (CF)</a></li>
<li class="toctree-l5"><a class="reference internal" href="#openshift-okd">OpenShift/OKD</a></li>
<li class="toctree-l5"><a class="reference internal" href="#heroku">Heroku</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#containers">Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#containers-vs-vm">Containers vs VM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#images-and-containers">Images and Containers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#container-technology-building-blocks">Container Technology: Building Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#namespaces">Namespaces</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#pid">pid</a></li>
<li class="toctree-l5"><a class="reference internal" href="#network">network</a></li>
<li class="toctree-l5"><a class="reference internal" href="#user">user</a></li>
<li class="toctree-l5"><a class="reference internal" href="#mnt">mnt</a></li>
<li class="toctree-l5"><a class="reference internal" href="#ipc">ipc</a></li>
<li class="toctree-l5"><a class="reference internal" href="#uts">uts</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#cgroups">cgroups</a></li>
<li class="toctree-l4"><a class="reference internal" href="#union-filesystem">Union filesystem</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#container-runtimes">Container Runtimes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#runc">runC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#containerd">containerd</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cri-o">CRI-O</a></li>
<li class="toctree-l4"><a class="reference internal" href="#docker">Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#project-moby">Project Moby</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#containers-micro-oses-for-containers">Containers: Micro OSes for Containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#alpine-linux">Alpine Linux</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fedora-coreos">Fedora CoreOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ubuntu-core">Ubuntu Core</a></li>
<li class="toctree-l4"><a class="reference internal" href="#photon-os">Photon OS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#containers-container-orchestration">Containers: Container Orchestration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#docker-swarm">Docker Swarm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kubernetes">Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#apache-mesos-dc-os-marathon">Apache Mesos/DC-OS/Marathon</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#apache-mesos">Apache Mesos</a></li>
<li class="toctree-l5"><a class="reference internal" href="#dc-os">DC/OS</a></li>
<li class="toctree-l5"><a class="reference internal" href="#marathon">Marathon</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#hashicorp-nomad">Hashicorp Nomad</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kubernetes-hosted-solutions">Kubernetes Hosted Solutions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cloud-container-orchestration-services">Cloud Container Orchestration Services</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#amazon-elastic-container-service">Amazon Elastic Container Service</a></li>
<li class="toctree-l5"><a class="reference internal" href="#azure-container-instances">Azure Container Instances</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#unikernels">Unikernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="#microservices">Microservices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#benefits">Benefits</a></li>
<li class="toctree-l3"><a class="reference internal" href="#disadvantages">Disadvantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#software-defined-networking">Software-Defined Networking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#networking-for-containers">Networking for containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#single-host">Single Host</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-host">Multi Host</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#container-networking-standards">Container Networking Standards</a></li>
<li class="toctree-l3"><a class="reference internal" href="#service-discovery">Service Discovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="#docker-networking">Docker Networking</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#single-host-networking">Single-Host Networking</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#bridge-network">Bridge Network</a></li>
<li class="toctree-l5"><a class="reference internal" href="#creating-a-network-bridge">Creating a network bridge</a></li>
<li class="toctree-l5"><a class="reference internal" href="#null-driver">Null Driver</a></li>
<li class="toctree-l5"><a class="reference internal" href="#host-driver">Host Driver</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#multi-host-networking">Multi-Host Networking</a></li>
<li class="toctree-l4"><a class="reference internal" href="#docker-plugins">Docker Plugins</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#kubernetes-networking">Kubernetes Networking</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cloud-foundry-container-to-container-networking">Cloud Foundry: Container to Container Networking</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#software-defined-storage-and-storage-management">Software-Defined Storage and Storage Management</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#software-defined-storage">Software-Defined Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ceph">Ceph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#glusterfs">GlusterFS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#storage-management-for-containers">Storage Management for Containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#docker-storage-backends">Docker Storage Backends</a></li>
<li class="toctree-l4"><a class="reference internal" href="#managing-data-in-docker">Managing Data in Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-a-container-with-volumes">Creating a container with volumes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-a-named-volume">Creating a named volume</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mounting-a-host-directory-inside-the-container">Mounting a host directory inside the container</a></li>
<li class="toctree-l4"><a class="reference internal" href="#volume-plugins-for-docker">Volume plugins for docker</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#volume-management-in-kubernetes">Volume Management in Kubernetes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#volume-types">Volume types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#persistant-volumes">Persistant Volumes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#persistent-volumes-claim">Persistent Volumes Claim</a></li>
<li class="toctree-l4"><a class="reference internal" href="#container-storage-interface-csi">Container Storage Interface (CSI)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cloud-foundry-volume-service">Cloud Foundry Volume Service</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#devops-and-ci-cd">DevOps and CI/CD</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#why-ci">Why CI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#definitions">Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ci-cd-tools">CI/CD Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#jenkins">Jenkins</a></li>
<li class="toctree-l4"><a class="reference internal" href="#travis-ci">Travis CI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#shippable">Shippable</a></li>
<li class="toctree-l4"><a class="reference internal" href="#consource">Consource</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ci-cd-kubernetes-tools">CI/CD Kubernetes Tools</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tools-for-cloud-infrastructure">Tools for Cloud Infrastructure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#configuration-management">Configuration Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ansible">Ansible</a></li>
<li class="toctree-l4"><a class="reference internal" href="#puppet">Puppet</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#puppet-agent">Puppet Agent</a></li>
<li class="toctree-l5"><a class="reference internal" href="#puppet-master">Puppet Master</a></li>
<li class="toctree-l5"><a class="reference internal" href="#catalog-file">Catalog File</a></li>
<li class="toctree-l5"><a class="reference internal" href="#puppet-tools">Puppet Tools</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#chef">Chef</a></li>
<li class="toctree-l4"><a class="reference internal" href="#salt-stack">Salt Stack</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#build-and-release">Build and Release</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#terraform">Terraform</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#terraform-providers">Terraform Providers</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#cloudformation">CloudFormation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bosh">BOSH</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#key-value-pair-store">Key-Value Pair Store</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#etcd">etcd</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#etcd-use-cases">etcd use-cases</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#consul">Consul</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#consul-use-cases">Consul use-cases</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#zookeeper">ZooKeeper</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#zookeeper-use-cases">Zookeeper use-cases</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#container-image-building">Container Image Building</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#docker-1">Docker</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#multi-stage-dockerfile">Multi-stage Dockerfile</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#packer">Packer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#debugging-logging-and-monitoring-for-containerized-applications">Debugging, Logging and Monitoring for Containerized Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#docker-debugging">Docker - Debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sysdig">Sysdig</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cadvisor">cAdvisor</a></li>
<li class="toctree-l4"><a class="reference internal" href="#elasticsearch">Elasticsearch</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fluentd">Fluentd</a></li>
<li class="toctree-l4"><a class="reference internal" href="#others-commercial">Others - Commercial</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#service-mesh">Service Mesh</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#features-and-implementation-of-service-mesh">Features and Implementation of Service Mesh</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-plane-and-control-plane">Data Plane and Control Plane</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">Consul</a></li>
<li class="toctree-l4"><a class="reference internal" href="#envoy">Envoy</a></li>
<li class="toctree-l4"><a class="reference internal" href="#istio">Istio</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#istio-architecture">Istio Architecture</a></li>
<li class="toctree-l5"><a class="reference internal" href="#istio-components">Istio Components</a></li>
<li class="toctree-l5"><a class="reference internal" href="#key-features-and-benefits">Key features and benefits</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#kuma">Kuma</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#kuma-architecture">Kuma Architecture</a></li>
<li class="toctree-l5"><a class="reference internal" href="#kuma-modes">Kuma Modes</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#linkerd">Linkerd</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mesh">Mesh</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#mesh-architecture">Mesh Architecture</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#tanzu-service-mesh-commercial">Tanzu Service Mesh - Commercial</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#internet-of-things">Internet of Things</a></li>
<li class="toctree-l2"><a class="reference internal" href="#serverless-computing">Serverless Computing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#aws-lambda">AWS Lambda</a></li>
<li class="toctree-l3"><a class="reference internal" href="#google-cloud-functions">Google Cloud Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#azure-functions">Azure Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#serverless-computing-1">Serverless Computing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#projects-that-use-containers-to-execute-serverless-applications">Projects That Use Containers to Execute Serverless Applications</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#distributed-tracing">Distributed Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hosting-providers-github-and-more">Hosting Providers: GitHub and More</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#github-repositories-types">GitHub Repositories - Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-git-interfaces-gerrit">Advanced Git Interfaces: Gerrit</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="LFF-ESS-P0E-OpenSource.html">Open Source Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="LFF-ESS-P0D-SecureSoftware.html">Secure Software Development Fundamentals</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Infrastructure Pentest</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Series_Infrastructure_Pentest/LFF-IPS-P1-IntelligenceGathering.html">Intelligence Gathering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Infrastructure_Pentest/LFF-IPS-P2-VulnerabilityAnalysis.html">Vulnerability Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Infrastructure_Pentest/LFF-IPS-P3-Exploitation.html">Exploitation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Infrastructure_Pentest/LFF-IPS-P4-PostExploitation.html">Post Exploitation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Infrastructure_Pentest/LFF-IPS-P5-Reporting.html">Reporting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Infrastructure_Pentest/LFF-IPS-P6-ConfigurationReview.html">Configuration Review</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vulnerable Machines</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Series_Vulnerable_Machines/LFC-VM-P0-InitialRecon.html">Initial Recon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Vulnerable_Machines/LFC-VM-P1-FromNothingToUnprivilegedShell.html">From Nothing to a Unprivileged Shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Vulnerable_Machines/LFC-VM-P2-UnprivilegedToPrivilegedShell.html">Unprivileged Shell to Privileged Shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Vulnerable_Machines/LFC-VM-P3-TipsAndTricks.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Vulnerable_Machines/LFC-VM-P4-Appendix.html">Appendix</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CTF - Challenges</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Series_Capture_The_Flag/LFC-BinaryExploitation.html">Binary Exploitation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Capture_The_Flag/LFC-Forensics.html">Forensics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Capture_The_Flag/LFC-ReverseEngineering.html">Reverse Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Capture_The_Flag/LFC-Cryptography.html">Cryptography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Capture_The_Flag/LFC-CodingQuickRef.html">Coding Quick Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Critical Infrastructure</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Series_Critical_Infrastructure/LFF-CIS-IndustrialControlSystems.html">Industrial Control Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Series_Critical_Infrastructure/LFF-CIS-ElectricalGrid.html">Electrical Grid</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">tech.bitvijays.com</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Cloud Infrastructure Technologies</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="cloud-infrastructure-technologies">
<h1>Cloud Infrastructure Technologies<a class="headerlink" href="#cloud-infrastructure-technologies" title="Link to this heading"></a></h1>
<p>The below notes are a mix of self-learning from internet and free Linux
Foundation course <a class="reference external" href="https://training.linuxfoundation.org/training/introduction-to-devops-and-site-reliability-engineering-lfs162/">Introduction to DevOps and Site Reliability
Engineering
(LFS162x)</a>
and <a class="reference external" href="https://training.linuxfoundation.org/training/introduction-to-cloud-infrastructure-technologies/">Introduction to Cloud Infrastructure Technologies
(LFS151x)</a>.</p>
<section id="initial-cloud-concepts">
<h2>Initial cloud concepts<a class="headerlink" href="#initial-cloud-concepts" title="Link to this heading"></a></h2>
<section id="cloud-computing">
<h3>Cloud computing<a class="headerlink" href="#cloud-computing" title="Link to this heading"></a></h3>
<p>Cloud computing can be defined as</p>
<ul class="simple">
<li><p>Computation performed on a remote machine.</p></li>
<li><p>The remote machine can be present on the company premises in another
branch/location or provided by cloud platform providers such as
<a class="reference external" href="https://cloud.google.com/">Google Cloud</a>/<a class="reference external" href="https://aws.amazon.com/">AWS
Cloud</a>/<a class="reference external" href="https://azure.microsoft.com/en-gb/">Microsoft
Azure</a>/<a class="reference external" href="https://www.digitalocean.com/">DigitalOcean</a>.</p></li>
</ul>
<p><a class="reference external" href="https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf">NIST-800-145</a>
defines cloud computing as <em>a model for enabling ubiquitous, convenient,
on-demand network access to a shared pool of configurable computing
resources (e.g., networks, servers, storage, applications, and services)
that can be rapidly provisioned and released with minimal management
effort or service provider interaction.</em></p>
<p>As the infrastructure (remote machines) are present remotely at someone
else’s premises, the end-users can benefit from speed and agility, cost,
easy access to resources, maintenance, multi-tenancy, and reliability.</p>
<p>Users can deploy cloud in different scenarios:</p>
<ul class="simple">
<li><p>Public cloud: Cloud platform such as <a class="reference external" href="https://cloud.google.com/">Google
Cloud</a>/<a class="reference external" href="https://aws.amazon.com/">AWS
Cloud</a>/<a class="reference external" href="https://azure.microsoft.com/en-gb/">Microsoft
Azure</a>/<a class="reference external" href="https://www.digitalocean.com/">DigitalOcean</a></p></li>
<li><p>Private cloud: Cloud hosted internally or externally and managed by
internal teams such as <a class="reference external" href="https://www.openstack.org/">Openstack</a></p></li>
<li><p>Hybrid cloud: Combination of public and private</p></li>
</ul>
<p>There could be different service models such as</p>
<ul class="simple">
<li><p>Infrastructure as a Service (IaaS)</p></li>
<li><p>Platform as a Service (PaaS)</p></li>
<li><p>Software as a Service (SaaS)</p></li>
</ul>
</section>
<section id="virtualization">
<h3>Virtualization<a class="headerlink" href="#virtualization" title="Link to this heading"></a></h3>
<p>Now, imagine that an administrator wants to deploy ten web servers.
There are multiple options:</p>
<ul class="simple">
<li><p>run them on different bare-metal machines (10 desktops with standard
specs such as 8GB RAM, 40 GB HDD/SDD, 4-Core CPU) or</p></li>
<li><p>run them on ten virtual machines running on one bare-metal machine
(one server with high specs such as 64/128 GB RAM, 2 TB SSD,
multi-core CPUs) or</p></li>
<li><p>run them on containers</p></li>
</ul>
<p>Officially, <a class="reference external" href="https://en.wikipedia.org/wiki/Virtualization">Wikipedia</a>
defines virtualization as an <em>act of creating a virtual (rather than
actual) version of something, including virtual computer hardware
platforms, operating systems, storage devices, and computer resources</em></p>
<p>Users can achieve virtualization at different hardware and software
layers, such as the Central Processing Unit (CPU), storage (disk),
memory (RAM), filesystems.</p>
<p>To perform the virtualization, we can use a Hypervisor.</p>
</section>
<section id="hypervisor">
<h3>Hypervisor<a class="headerlink" href="#hypervisor" title="Link to this heading"></a></h3>
<p>Hypervisor is defined as software that performs virtualization.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Hypervisor">Wikipedia</a> defines
<em>hypervisor (or virtual machine monitor, VMM, virtualizer) as a kind of
emulator; it is computer software, firmware, or hardware that creates
and runs virtual machines.</em></p>
<p>There are two types of hypervisors:</p>
<ul class="simple">
<li><p>Type-1, native or bare-metal hypervisors</p></li>
<li><p>Type-2, hosted hypervisors</p></li>
</ul>
<section id="type-1-native-or-bare-metal-hypervisors">
<h4>Type-1, native or bare-metal hypervisors<a class="headerlink" href="#type-1-native-or-bare-metal-hypervisors" title="Link to this heading"></a></h4>
<p>Hypervisors that run directly on the host’s hardware to control the
hardware and manage guest operating systems are called bare-metal
hypervisors or Type-1 hypervisors.</p>
<p>Examples are Microsoft
<a class="reference external" href="https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/about/">Hyper-V</a>,
<a class="reference external" href="https://www.vmware.com/uk/products/vsphere-hypervisor.html">VMware
ESXi</a>,
Xen, Nutanix AHV, XCP-ng, Oracle VM Server for SPARC, Oracle VM Server
for x86, POWER Hypervisor, and QNX Hypervisor.</p>
</section>
<section id="type-2-or-hosted-hypervisors">
<h4>Type-2 or hosted hypervisors<a class="headerlink" href="#type-2-or-hosted-hypervisors" title="Link to this heading"></a></h4>
<p>Type-2 hypervisor (hosted) runs on top of the host’s OS, mainly used by
end-users. For instance, running a Windows/Linux OS and running a
hypervisor on top of it.</p>
<p>Examples include <a class="reference external" href="https://www.virtualbox.org/">VirtualBox</a>, VMware
Player, VMware Workstation, Parallels Desktop for Mac.</p>
</section>
<section id="type-1-2-hypervisor-example-linux-kvm">
<h4>Type-1/2 Hypervisor Example: Linux KVM<a class="headerlink" href="#type-1-2-hypervisor-example-linux-kvm" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.linux-kvm.org/">Linux-KVM</a> defines <em>KVM (for
Kernel-based Virtual Machine) as a full virtualization solution for
Linux on x86 hardware containing virtualization extensions (Intel VT or
AMD-V)</em>.</p>
<p>KVM is a loadable virtualization module of the Linux kernel. It converts
the kernel into a hypervisor capable of managing guest Virtual Machines.</p>
<p>Debian has provided instructions to <a class="reference external" href="https://wiki.debian.org/KVM">install KVM on
Debian</a></p>
<section id="manage-kvm-vms">
<h5>Manage KVM VMs<a class="headerlink" href="#manage-kvm-vms" title="Link to this heading"></a></h5>
<p>There are multiple options to <a class="reference external" href="https://www.linux-kvm.org/page/Management_Tools">manage KVM Virtual
Machines</a></p>
<ul class="simple">
<li><p><a class="reference external" href="http://libvirt.org/">virsh</a>: A minimal shell around libvirt for
managing VMs.</p></li>
<li><p><a class="reference external" href="http://virt-manager.org/">VMM/Virtual Machine Manager</a>: Also
known as virt-manager. A desktop user interface for managing virtual
machines.</p></li>
<li><p><a class="reference external" href="http://cloudstack.org/">Cloudstack</a>: Cloudstack is an open-source
project that enables the deployment, management, and configuration of
multi-tier and multi-tenant infrastructure cloud services using Xen,
KVM, and VMware hypervisors.</p></li>
<li><p><a class="reference external" href="https://pve.proxmox.com/wiki/">Proxmox VE</a>: Proxmox Virtual
Environment (Proxmox VE) is an open-source server virtualization
management platform to manage VMs and containers.</p></li>
</ul>
<p>Linux KVM can work as a type-1 and type-2 hypervisor.</p>
</section>
</section>
<section id="type-2-hypervisor-example-virtualbox">
<h4>Type-2 Hypervisor Example: Virtualbox<a class="headerlink" href="#type-2-hypervisor-example-virtualbox" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.virtualbox.org/">VirtualBox</a> is an x86 and AMD64/Intel64
virtualization product from Oracle (open-source, free to use), running
on Windows, Linux, Mac OS X, and Solaris hosts and supporting various
guest OSes.</p>
<p>It provides two virtualization choices: software-based virtualization
and hardware-assisted virtualization. It provides the ability to run
virtualized applications side-by-side with normal desktop applications.</p>
</section>
</section>
<section id="vm-management">
<h3>VM Management<a class="headerlink" href="#vm-management" title="Link to this heading"></a></h3>
<p>Now, using Virtualbox, VMWare is good. However, the administrator still
has to manually install the OS, such as selecting the RAM, hard disk
size, keyboard layout, and other settings, which is quite
time-consuming. Configuring and sharing one VM is easy; however,
managing multiple VM and manually performing all the build and
configuration steps can be complicated.</p>
<p>VM Management allows to automate the setup of one or more VMs, resulting
in saved time, increased productivity, and lower operational costs.</p>
<section id="vagrant">
<h4>Vagrant<a class="headerlink" href="#vagrant" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.vagrantup.com/">Vagrant</a> allows VM management and allows
to have consistency in infrastructure provisioning. Vagrant helps to
automate VMs management by providing an end-to-end lifecycle management
utility - the <code class="docutils literal notranslate"><span class="pre">vagrant</span></code> command-line tool.</p>
<p>Using virtual machines in a development environment allows us to have a
reproducible environment and share the environment with other teammates.</p>
<p>Vagrant defines the virtual machines in a <code class="docutils literal notranslate"><span class="pre">Vagrantfile</span></code>.</p>
<section id="vagrant-file">
<h5>Vagrant file<a class="headerlink" href="#vagrant-file" title="Link to this heading"></a></h5>
<p>The <a class="reference external" href="https://www.vagrantup.com/docs/vagrantfile">Vagrantfile</a></p>
<ul class="simple">
<li><p>describes how VM should be configured and provisioned.</p></li>
<li><p>It is a text file with the Ruby syntax providing information about
configuring and provisioning a set of machines.</p></li>
<li><p>It includes information about the machine type, image, networking,
provider-specific information, and provisioner details.</p></li>
</ul>
<p>Now, if we consider what information should be present in the
<code class="docutils literal notranslate"><span class="pre">Vagrantfile</span></code></p>
<ul class="simple">
<li><p>Boxes: should contain what OS needs to be run.</p></li>
<li><p>how the access to the OS is provided (ssh/RDP/vnc)</p></li>
<li><p>Networking: how the network is configured (host-only/bridge/NAT)</p></li>
<li><p>Providers: which hypervisor is used to provide the configuration
(Virtualbox/Hyper-V/KVM)</p></li>
<li><p>Provisioning: how the machine is provisioned when the machine is
booted first.</p></li>
<li><p>Synced Folders: OS might require to share data (folders) between the
host OS and the VM.</p></li>
</ul>
</section>
<section id="boxes">
<h5>Boxes<a class="headerlink" href="#boxes" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://www.vagrantup.com/docs/boxes.html">Boxes</a> are the package
format for the Vagrant environment. The Vagrantfile requires an image
used to instantiate VM. If the image is not available locally, vagrant
downloads it from a central image repository such as the <a class="reference external" href="https://app.vagrantup.com/boxes/search">Vagrant Cloud
box repository</a>. Box images
can be versioned and customized to specific needs simply by updating the
Vagrantfile accordingly.</p>
<p>For instance:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://app.vagrantup.com/ubuntu">Ubuntu Boxes</a></p></li>
<li><p><a class="reference external" href="https://app.vagrantup.com/debian">Debian Boxes</a></p></li>
</ul>
</section>
<section id="providers">
<h5>Providers<a class="headerlink" href="#providers" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://www.vagrantup.com/docs/providers/">Providers</a> are the
underlying engines or hypervisors used to provision VMs or containers.</p>
<p>Vagrant has VirtualBox as a default provider and supports Hyper-V,
Docker, VMWare hypervisors. Custom providers such as KVM, AWS may be
installed as well.</p>
</section>
<section id="synced-folders">
<h5>Synced Folders<a class="headerlink" href="#synced-folders" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://www.vagrantup.com/docs/synced-folders/">Synced Folder</a>
allows syncing a directory on the host system with a VM, which helps the
user manage shared files/directories easily.</p>
</section>
<section id="provisioning">
<h5>Provisioning<a class="headerlink" href="#provisioning" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://www.vagrantup.com/docs/provisioning/">Provisioners</a> allow us
to automatically install software and make configuration changes after
the machine is booted as a part of the <code class="docutils literal notranslate"><span class="pre">vagrant</span> <span class="pre">up</span></code> process.</p>
<p>Many types of provisioners are available, such as</p>
<ul class="simple">
<li><p>File,</p></li>
<li><p>Shell,</p></li>
<li><p>Ansible/Puppet/Chef/Salt</p></li>
<li><p>Docker,</p></li>
</ul>
<p>For example, Shell as the provisioner to install the vim package.</p>
<div class="highlight-ruby notranslate"><div class="highlight"><pre><span></span><span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">provision</span><span class="w"> </span><span class="s2">&quot;shell&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">inline</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;&lt;-</span><span class="dl">SHELL</span>
<span class="sh">              yum install vim -y</span>
<span class="dl">   SHELL</span>
</pre></div>
</div>
</section>
<section id="plugins">
<h5>Plugins<a class="headerlink" href="#plugins" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://www.vagrantup.com/docs/plugins/">Plugins</a> extend the
functionality of vagrant.</p>
</section>
<section id="networking">
<h5>Networking<a class="headerlink" href="#networking" title="Link to this heading"></a></h5>
<p>Vagrant provides high-level
<a class="reference external" href="https://www.vagrantup.com/docs/networking/">networking</a> options for
port forwarding, network connectivity, and network creation.</p>
</section>
<section id="multi-machine">
<h5>Multi-Machine<a class="headerlink" href="#multi-machine" title="Link to this heading"></a></h5>
<p>A project’s Vagrantfile may describe <a class="reference external" href="https://www.vagrantup.com/docs/multi-machine/">multiple
VMs</a>, which are
typically intended to work together or maybe linked between themselves.</p>
</section>
</section>
</section>
</section>
<section id="deployment-models">
<h2>Deployment Models<a class="headerlink" href="#deployment-models" title="Link to this heading"></a></h2>
<section id="infrastructure-as-a-service">
<h3>Infrastructure as a Service<a class="headerlink" href="#infrastructure-as-a-service" title="Link to this heading"></a></h3>
<p>Infrastructure as a Service (IaaS) is a cloud service model that
provides on-demand physical and virtual computing resources, storage,
network, firewall, and load balancers. To provide virtual computing
resources, IaaS uses hypervisors, such as Xen, KVM, VMware ESXi,
Hyper-V, or Nitro.</p>
<section id="iaas-providers">
<h4>IaaS providers<a class="headerlink" href="#iaas-providers" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://aws.amazon.com/ec2/">Amazon EC2</a></p></li>
<li><p><a class="reference external" href="https://azure.microsoft.com/en-gb/services/virtual-machines/">Azure Virtual
Machines</a></p></li>
<li><p><a class="reference external" href="https://www.digitalocean.com/products/droplets/">Digital Ocean
Droplet</a></p></li>
<li><p><a class="reference external" href="https://cloud.google.com/compute">Google Compute Engine</a></p></li>
<li><p><a class="reference external" href="https://www.ibm.com/uk-en/cloud/virtual-servers">IBM Cloud Virtual
Servers</a></p></li>
<li><p><a class="reference external" href="https://www.oracle.com/uk/cloud/compute/virtual-machines/">Oracle Cloud Compute Virtual
Machines</a></p></li>
<li><p>OpenStack</p></li>
</ul>
</section>
</section>
<section id="platform-as-a-service">
<h3>Platform as a Service<a class="headerlink" href="#platform-as-a-service" title="Link to this heading"></a></h3>
<p>Platform as a Service (PaaS) is a cloud service model representing a
class of cloud computing services that allow users to develop, run, and
manage applications rather than manage the underlying infrastructure.</p>
<p>Users have a choice between managed and self-managed PaaS solutions.
Users can either use</p>
<ul class="simple">
<li><p>managed PaaS solutions hosted by cloud computing providers.</p></li>
</ul>
<p>or deploy an</p>
<ul class="simple">
<li><p>on-premise PaaS as a self-managed solution, using a platform such as
Red Hat OpenShift/OKD.</p></li>
</ul>
<p>PaaS can be deployed on top of IaaS or independently on VMs, bare-metal
servers, and containers.</p>
<section id="paas-providers">
<h4>PaaS providers<a class="headerlink" href="#paas-providers" title="Link to this heading"></a></h4>
<section id="cloud-foundry-cf">
<h5>Cloud Foundry (CF)<a class="headerlink" href="#cloud-foundry-cf" title="Link to this heading"></a></h5>
<p>CF is an open-source PaaS framework. It provides a highly efficient,
modern model for cloud-native application delivery on top of Kubernetes.</p>
<p>CF provides application portability, auto-scaling, isolation,
centralized platform management, centralized logging, dynamic routing,
application health management, role-based application deployment,
horizontal and vertical scaling, security, support for different IaaS
platforms.</p>
<p>Cloud Foundry Runtimes</p>
<p>CF utilizes two runtimes to manage and run applications and containers
separately. Both runtimes are managed by <a class="reference external" href="https://bosh.io/docs/">CF
BOSH</a>. BOSH is a cloud-agnostic
open-source tool for release engineering, deployment, and lifecycle
management of complex distributed systems.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.cloudfoundry.org/application-runtime/">Cloud Foundry Application Runtime
(CFAR)</a> allows
developers to run applications written in any language or framework
on the cloud of their choice. It uses
<a class="reference external" href="https://docs.cloudfoundry.org/buildpacks/">buildpacks</a> to provide
the framework and runtime support for applications.</p>
<ul>
<li><p>Buildpacks are programming language-specific and include
information on how to download dependencies and configure specific
applications.</p></li>
<li><p>When an application is pushed, CF automatically detects an
appropriate buildpack for it to compile or prepare the application
for launch.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.cloudfoundry.org/application-runtime/">Cloud Foundry Container Runtime
(CFCR)</a> offers
users to deploy cloud-native, developer-built, pre-packaged
applications in containers. The CFCR platform manages containers on a
Kubernetes cluster (managed by CF BOSH).</p></li>
</ul>
<p>KubeCF</p>
<p><a class="reference external" href="https://kubecf.io/">KubeCF</a> is a distribution of
CFAR optimized for Kubernetes. It allows developers on CFAR to enjoy the
benefits of Kubernetes, just as developers using CFCR do.</p>
<p>KubeCF operates in conjunction with two incubating projects of CF:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://quarks.suse.dev/">Project Quarks</a> is an incubating effort
within the CF Foundation to integrate CF and Kubernetes. It packages
the CFAR as containers instead of virtual machines, enabling easy
deployment to Kubernetes. The resulting containerized CFAR provides
an equivalent developer experience to that of BOSH-managed Cloud
Foundry installations, requires less infrastructure capacity, and
delivers an operational experience familiar to Kubernetes operators.</p></li>
<li><p><a class="reference external" href="https://github.com/cloudfoundry/eirini/">Project Eirini</a> is a Kubernetes backend for
CF. It deploys CF apps to a Kube backend, using OCI images and Kube
deployments.</p></li>
</ul>
</section>
<section id="openshift-okd">
<h5>OpenShift/OKD<a class="headerlink" href="#openshift-okd" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>OpenShift is an open-source PaaS solution provided by Red Hat. It is
built on top of the container technology orchestrated by Kubernetes.</p></li>
<li><p><a class="reference external" href="https://www.okd.io/">OKD</a> is the Community Distribution of
Kubernetes that powers Red Hat’s OpenShift.</p></li>
<li><p>OKD is a distribution of Kubernetes optimized for continuous
application development and multi-tenant deployment. OKD adds
developer and operations-centric tools on top of Kubernetes to enable
rapid application development, easy deployment and scaling, and
long-term lifecycle maintenance for small and large teams.</p></li>
</ul>
<p>Users can deploy OpenShift</p>
<ul class="simple">
<li><p>in the cloud or</p></li>
<li><p>locally on a full-fledged Linux OS or</p></li>
<li><p>on a Micro OS specifically designed to run containers and Kubernetes.</p></li>
</ul>
<p>OpenShift uses the Source-to-Image (S2I) framework that enables users to
build container images from the source code repository allowing for fast
and easy application deployments. OpenShift integrates well with
Continuous Deployment tools to deploy applications as part of the CI/CD
pipeline. Applications are managed with ease through command line (CLI)
tools, web user interface (UI), and integrated development environment
(IDE) available for OpenShift.</p>
</section>
<section id="heroku">
<h5>Heroku<a class="headerlink" href="#heroku" title="Link to this heading"></a></h5>
<p>Heroku is a fully managed container-based cloud platform with integrated
data services and a strong ecosystem. Heroku is used to deploy and run
modern applications. The Heroku Platform supports the popular languages
and frameworks: Node.js, Ruby, Python, Go, PHP, Clojure, Scala, Java.</p>
<p>To learn deployment on an application on the Heroku platform, follow the
<a class="reference external" href="https://devcenter.heroku.com/start">Getting Started on Heroku</a>, a
step-by-step guide for deploying your first app and mastering the basics
of Heroku.</p>
</section>
</section>
</section>
</section>
<section id="containers">
<h2>Containers<a class="headerlink" href="#containers" title="Link to this heading"></a></h2>
<p>Previously, when an administrator wanted to deploy ten web applications,
they might create ten virtual machines and run applications. Further,
they may also choose to deploy ten applications on a single machine.
However, when multiple applications are deployed on one host, developers
are faced with the challenge of isolating applications from one another
to avoiding conflicts between dependencies, libraries, and runtimes.</p>
<p>Also, a developer’s end goal is to create a portable application that
works consistently on multiple hardware and platforms, such as the
developer’s laptop, VMs, data centers, public and private clouds, from
the development environment to production. However, they usually end up
in situations that the applications work on their laptop but not on
another machine. Using a container technology like Docker, users can
bundle the application and its dependencies in a box. Users can ship the
box to different platforms, and it will run identically on each one of
them.</p>
<p><a class="reference external" href="https://www.docker.com/resources/what-container">Docker - What is a
Container</a> defines a
container as a standard unit of software that packages up code and all
its dependencies, so the application runs quickly and reliably from one
computing environment to another.</p>
<section id="containers-vs-vm">
<h3>Containers vs VM<a class="headerlink" href="#containers-vs-vm" title="Link to this heading"></a></h3>
<p>A virtual machine runs on top of a hypervisor, which virtualizes a host
system’s hardware by emulating CPU, memory, and networking hardware
resources so that a guest OS can be installed on top of them. Between an
application running inside a guest OS and in the outside world, there
are multiple layers:</p>
<ul class="simple">
<li><p>the guest OS,</p></li>
<li><p>the hypervisor, and</p></li>
<li><p>the host OS.</p></li>
</ul>
<p>In contrast to VMs, containers run directly as processes on the host OS.
There is no indirection as we see in VMs, which helps containers to
achieve near-native performance. Also, as the containers have a very
light footprint, it becomes easier to pack more containers than VMs on
the same physical machine.</p>
<p>When running containers, we have to ensure that container OS is same as
the host OS as it is operating system level virtualization. For
instance, Linux containers can only be run over a Linux host OS; Windows
containers can only be run over a Windows host OS; Linux containers can
not be executed over Windows host OS or vice-versa.</p>
<p><a class="reference external" href="https://www.docker.com/resources/what-container">Docker</a> presents
how the application is deployed in containers and VMs.</p>
</section>
<section id="images-and-containers">
<h3>Images and Containers<a class="headerlink" href="#images-and-containers" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>A Docker container image is a lightweight, standalone, executable
package of software that includes everything needed to run an
application: code, runtime, system tools, system libraries, and
settings.</p></li>
<li><p>A running instance of the above image is referred to as a container.
We can run multiple containers from the same image.</p></li>
<li><p>An image contains the application, its dependencies, and the
user-space libraries. User-space libraries like <code class="docutils literal notranslate"><span class="pre">glibc</span></code> enable
switching from the user-space to the kernel-space. An image does not
contain any kernel-space components.</p></li>
<li><p>When a container is created from an image, it runs as a process on
the host’s kernel. The host kernel’s job is to isolate the container
process and provide resources to each container.</p></li>
</ul>
<p><a class="reference external" href="https://twitter.com/b0rk">Julia Evans</a> has provided a good summary of what’s a container.</p>
<figure class="align-default" id="id5">
<img alt="What's a container" src="../_images/whatis_container.jpeg" />
<figcaption>
<p><span class="caption-text">What’s a container</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="container-technology-building-blocks">
<h3>Container Technology: Building Blocks<a class="headerlink" href="#container-technology-building-blocks" title="Link to this heading"></a></h3>
<p>The Linux kernel provides basic building blocks of container technology.</p>
<section id="namespaces">
<h4>Namespaces<a class="headerlink" href="#namespaces" title="Link to this heading"></a></h4>
<p>A
<a class="reference external" href="https://man7.org/linux/man-pages/man7/namespaces.7.html">namespace</a>
wraps a global system resource in an abstraction that makes it appear to
the processes within the namespace that they have their own isolated
instance of the global resource.</p>
<p>Julia Evans has provided a good summary of what’s a namespace.</p>
<figure class="align-default" id="id6">
<img alt="What's a Namespace" src="../_images/whatis_namespaces_1.jpeg" />
<figcaption>
<p><span class="caption-text">What’s a Namespace</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-default" id="id7">
<img alt="What's a Namespace" src="../_images/whatis_namespaces_2.jpeg" />
<figcaption>
<p><span class="caption-text">What’s a Namespace</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The following global resources are namespaced:</p>
<section id="pid">
<h5>pid<a class="headerlink" href="#pid" title="Link to this heading"></a></h5>
<p>Provides each namespace to have the same PIDs. Each container has its
own PID 1.</p>
<figure class="align-default" id="id8">
<img alt="What's a PID Namespace" src="../_images/whatis_pid_namespaces.jpeg" />
<figcaption>
<p><span class="caption-text">What’s a PID Namespace</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="network">
<h5>network<a class="headerlink" href="#network" title="Link to this heading"></a></h5>
<p>Allows each namespace to have its network stack. Each container has its
own IP address.</p>
<figure class="align-default" id="id9">
<img alt="What's a Network Namespace" src="../_images/network-namespaces.png" />
<figcaption>
<p><span class="caption-text">What’s a Network Namespace</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="user">
<h5>user<a class="headerlink" href="#user" title="Link to this heading"></a></h5>
<p>Allows each namespace to have its user and group ID number spaces. A
root user inside a container is not the host’s root user on which the
container is running.</p>
<figure class="align-default" id="id10">
<img alt="What's a User Namespace" src="../_images/whatis_user_namespaces.jpeg" />
<figcaption>
<p><span class="caption-text">What’s a User Namespace</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="mnt">
<h5>mnt<a class="headerlink" href="#mnt" title="Link to this heading"></a></h5>
<p>Allows each namespace to have its own view of the filesystem hierarchy.</p>
</section>
<section id="ipc">
<h5>ipc<a class="headerlink" href="#ipc" title="Link to this heading"></a></h5>
<p>Allows each namespace to have its own interprocess communication.</p>
</section>
<section id="uts">
<h5>uts<a class="headerlink" href="#uts" title="Link to this heading"></a></h5>
<p>Allows each namespace to have its own hostname and domain name.</p>
</section>
</section>
<section id="cgroups">
<h4>cgroups<a class="headerlink" href="#cgroups" title="Link to this heading"></a></h4>
<p>Control groups organize processes hierarchically and distribute system
resources along the hierarchy in a controlled and configurable manner.
The following cgroups are available for Linux:</p>
<ul class="simple">
<li><p>blkio</p></li>
<li><p>cpu</p></li>
<li><p>cpuacct</p></li>
<li><p>cpuset</p></li>
<li><p>devices</p></li>
<li><p>freezer</p></li>
<li><p>memory.</p></li>
</ul>
<figure class="align-default" id="id11">
<img alt="What's a cgroup" src="../_images/whatis_cgroups.jpeg" />
<figcaption>
<p><span class="caption-text">What’s a cgroup</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="union-filesystem">
<h4>Union filesystem<a class="headerlink" href="#union-filesystem" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>The Union filesystem allows files and directories of separate
filesystems, known as layers, transparently overlaid on top of each
other to create a new virtual filesystem.</p></li>
<li><p>An image used in Docker is made of multiple layers. While starting a
new container, we merge all those layers to create a read-only
filesystem.</p></li>
<li><p>On top of a read-only filesystem, a container gets a read-write
layer, an ephemeral layer, and local to the container.</p></li>
</ul>
</section>
</section>
<section id="container-runtimes">
<h3>Container Runtimes<a class="headerlink" href="#container-runtimes" title="Link to this heading"></a></h3>
<p>Now, we have learned about container images. These images need to be
executed and running for becoming a container. To run them, we need a
container runtime. The container runtime ensures container portability,
offering a consistent environment for containers to run, regardless of
the infrastructure.</p>
<section id="runc">
<h4>runC<a class="headerlink" href="#runc" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://opencontainers.org/">Open Container Initiative (OCI)</a>
ensures that there is no vendor locking and no inclination towards a
particular company or project. The governance body came up with both
<a class="reference external" href="https://github.com/opencontainers/runtime-spec">runtime</a> and
<a class="reference external" href="https://github.com/opencontainers/image-spec">image</a> specifications
to create standards on the Operating System process and application
containers.</p>
<p><a class="reference external" href="https://github.com/opencontainers/runc">runC</a> is the CLI tool for
spawning and running containers.</p>
</section>
<section id="containerd">
<h4>containerd<a class="headerlink" href="#containerd" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://containerd.io/">containerd</a> is an OCI-compliant container
runtime emphasizing simplicity, robustness, and portability. It runs as
a daemon and manages the entire lifecycle of containers. Docker uses
containerd as a container runtime to manage runC containers.</p>
</section>
<section id="cri-o">
<h4>CRI-O<a class="headerlink" href="#cri-o" title="Link to this heading"></a></h4>
<p><a class="reference external" href="http://cri-o.io/">CRI-O</a> is an OCI-compatible runtime, which is an
implementation of the Kubernetes Container Runtime Interface (CRI). It
is a lightweight alternative to using Docker as the runtime for
Kubernetes.</p>
</section>
<section id="docker">
<h4>Docker<a class="headerlink" href="#docker" title="Link to this heading"></a></h4>
<p>Docker Containerization Platform runs applications using containers. It
comes in two versions:</p>
<ul class="simple">
<li><p>Docker Enterprise: It is a paid, enterprise-ready container platform
created to run and manage containers, currently part of Mirantis.</p></li>
<li><p>Docker Engine - Community: It is a free platform used to run and
manage containers.</p></li>
</ul>
<p>Docker has a client-server architecture, with a Docker Client connecting
to a Docker Host server to execute commands.</p>
</section>
<section id="project-moby">
<h4>Project Moby<a class="headerlink" href="#project-moby" title="Link to this heading"></a></h4>
<p>A container platform like Docker runs on different platforms and
architectures: bare metal (both x86 and ARM), Linux, Mac OS-X, and
Windows.</p>
<p>From the user perspective, the experience is seamless, regardless of the
underlying platform. However, behind the scenes, any container platform
contains multiple components such as container runtime, networking, and
storage connected to provide a high-quality experience.</p>
<p>However, what if the end-user wants to create their container platform
from individual components and build a container platform like Docker?
The end-user can create their platform by using <a class="reference external" href="https://mobyproject.org/">Project
Moby</a>.</p>
<p>Project Moby is an open-source project that provides a framework for
assembling different container systems to build a container platform
like Docker. Individual container systems provide features such as
image, container, and secret management. Moby is beneficial if the user
wants to build a container-based system or experiment with the latest
container technologies. It is not recommended for application developers
and newbies looking for an easy way to run containers.</p>
<p><a class="reference external" href="https://github.com/linuxkit/linuxkit">LinuxKit</a> is a toolkit for
building secure, portable, and lean operating systems for containers
uses Moby.</p>
</section>
</section>
<section id="containers-micro-oses-for-containers">
<h3>Containers: Micro OSes for Containers<a class="headerlink" href="#containers-micro-oses-for-containers" title="Link to this heading"></a></h3>
<p>Having understood the working of containers and running applications in
containers, it makes sense to run containers on secure, portable, and
lean operating systems. Users can create a minimal OS for running
containers by eliminating all the packages and services of the host OS,
which are not essential for running containers.</p>
<p>Vendors have created specialized minimal OSes to run just containers.
Once we remove the packages that are not essential to boot the base OS
and run container-related services, we are left with specialized OSes,
referred to as Micro OSes for containers. Some examples of Micro OSes
are:</p>
<ul class="simple">
<li><p>Alpine Linux</p></li>
<li><p>Fedora CoreOS (formerly known as Red Hat CoreOS)</p></li>
<li><p>Ubuntu Core</p></li>
<li><p>VMware Photon.</p></li>
</ul>
<section id="alpine-linux">
<h4>Alpine Linux<a class="headerlink" href="#alpine-linux" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.alpinelinux.org/about/">Alpine Linux</a> is a
security-oriented, lightweight Linux distribution based on musl libc and
busybox. It uses its own package manager called <code class="docutils literal notranslate"><span class="pre">apk</span></code>, the OpenRC init
system, script-driven setups. Alpine Linux was designed with security in
mind. All userland binaries are compiled as Position Independent
Executables (PIE) with stack smashing protection. It requires 8 MB
running as a container. It requires 130 MB as a standalone minimal OS
installation.</p>
<p>Upon <a class="reference external" href="https://wiki.alpinelinux.org/wiki/Installation">installation</a>
completion, Alpine Linux makes available tools for the system’s initial
configuration. Once prepared for a reboot, it can be configured to boot
in one of the three available runtime modes:</p>
<ul class="simple">
<li><p>diskless mode: The default mode, where the entire system runs from
RAM.</p></li>
<li><p>data mode: Mostly runs from RAM but mounts <code class="docutils literal notranslate"><span class="pre">/var</span></code> as a writable
data partition.</p></li>
<li><p>sys mode: The typical hard-disk install that mounts <code class="docutils literal notranslate"><span class="pre">/boot</span></code>,
<code class="docutils literal notranslate"><span class="pre">swap</span></code>, and <code class="docutils literal notranslate"><span class="pre">/</span></code>.</p></li>
</ul>
</section>
<section id="fedora-coreos">
<h4>Fedora CoreOS<a class="headerlink" href="#fedora-coreos" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://getfedora.org/en/coreos?stream=stable">Fedora CoreOS (FCOS)</a>
is an automatically updating, minimal, monolithic, container-focused
operating system, designed for clusters but also operable standalone,
optimized for Kubernetes. It aims to combine the best of both CoreOS
Container Linux and Fedora Atomic Host (FAH), integrating technology
like Ignition from Container Linux with <code class="docutils literal notranslate"><span class="pre">rpm-ostree</span></code> and <code class="docutils literal notranslate"><span class="pre">SELinux</span></code>
hardening from Project Atomic. Its goal is to provide the best container
host to run containerized workloads securely and at scale.</p>
<p><a class="reference external" href="https://docs.fedoraproject.org/en-US/fedora-coreos/faq/">Fedora CoreOS Frequently Asked
Questions</a>
and <a class="reference external" href="https://docs.fedoraproject.org/en-US/fedora-coreos/getting-started/">Getting Started with Fedora
CoreOS</a>
is a good way to understand more about Fedora CoreOS, and its
relationship with Atomic Host/RedHat CoreOS.</p>
</section>
<section id="ubuntu-core">
<h4>Ubuntu Core<a class="headerlink" href="#ubuntu-core" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://ubuntu.com/core">Ubuntu Core</a> is a version of Ubuntu
optimized for IoT-native embedded systems. Ubuntu Core carries only
packages and binaries for your single-purpose appliance. Ubuntu Core is
a container OS built on snaps. With snaps, embedded systems benefit from
security, immutability, as well as modularity, and composability.
Software is updated over the air through deltas that can automatically
roll back in case of failure.</p>
<p>It is essential to read about <a class="reference external" href="https://ubuntu.com/core">Ubuntu Core</a>,
<a class="reference external" href="https://ubuntu.com/core/docs">documentation</a>,
<a class="reference external" href="https://ubuntu.com/tutorials?q=core">tutorials</a> and the whitepapers
explaining various security features such as full-disk encryption,
application confinement, bare essentials packages, automatic scanning of
all snaps for vulnerable libraries and problematic code, secure boot.</p>
</section>
<section id="photon-os">
<h4>Photon OS<a class="headerlink" href="#photon-os" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://vmware.github.io/photon/">Photon OS</a> is a Linux-based, open-source, security-hardened,
enterprise-grade appliance operating system purpose-built for Cloud and
Edge applications.</p>
<p>Photon OS provides a lightweight container host; It follows the
recommendation of <a class="reference external" href="https://www.kernel.org/doc/html/latest/security/self-protection.html">Kernel Self-Protection Project
(KSPP)</a>
by building the packages with hardened security flags and can be easily
updated and verified. More information can be read using the
<a class="reference external" href="https://vmware.github.io/photon/docs/">Documentation</a></p>
</section>
</section>
<section id="containers-container-orchestration">
<h3>Containers: Container Orchestration<a class="headerlink" href="#containers-container-orchestration" title="Link to this heading"></a></h3>
<p>We have understood containers, the OSs that are optimized to run
containers. Till now, we have covered running containers on a single
node. However, there might be a case where containerized workload needs
to be managed at scale. To run containers in a multi-host environment at
scale, there could be multiple issues such as:</p>
<ul class="simple">
<li><p>How to group multiple hosts together to form a cluster and manage
them as a single compute unit?</p></li>
<li><p>How to schedule containers to run on specific hosts?</p></li>
<li><p>How can containers running on one host communicate with containers
running on other hosts?</p></li>
<li><p>How to provide the container with dependent storage when it is
scheduled on a specific host?</p></li>
<li><p>How to access containers over a service name instead of accessing
them directly through their IP addresses?</p></li>
</ul>
<p>Container orchestration tools and different plugins (for networking and
storage) can perform container scheduling and cluster management.
Container scheduling allows us to decide which host a container or a
group of containers should be deployed. With cluster management
orchestrators, we can manage the resources of cluster nodes and add or
delete nodes from the cluster. Few container orchestration tools are:</p>
<ul class="simple">
<li><p>Docker Swarm</p></li>
<li><p>Kubernetes</p></li>
<li><p>Mesos Marathon</p></li>
<li><p>Nomad</p></li>
</ul>
<section id="docker-swarm">
<h4>Docker Swarm<a class="headerlink" href="#docker-swarm" title="Link to this heading"></a></h4>
<p>Current versions of Docker include <a class="reference external" href="https://docs.docker.com/engine/swarm/">swarm
mode</a> for natively managing a
cluster of Docker Engines called a swarm. Use the Docker CLI to create a
swarm, deploy application services to a swarm, and manage swarm
behavior.</p>
<p>It would be a good idea to read about the <a class="reference external" href="https://docs.docker.com/engine/swarm/key-concepts/">Swarm key
concepts</a> and
then follow the <a class="reference external" href="https://docs.docker.com/engine/swarm/swarm-tutorial/">Getting started with swarm
mode</a></p>
</section>
<section id="kubernetes">
<h4>Kubernetes<a class="headerlink" href="#kubernetes" title="Link to this heading"></a></h4>
<p>Kubernetes, also known as K8s, is an open-source system for automating
deployment, scaling, and management of containerized applications. It
groups containers that make up an application into logical units for
easy management and discovery. Kubernetes supports several container
runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).</p>
<p><a class="reference external" href="https://kubernetes.io/docs/concepts/">Kubernetes Concepts</a> has all
the documentation we need. It might be a good idea to start the reading
from</p>
<ul class="simple">
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">What is
Kubernetes?</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes
Components</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/architecture/">Cluster
Architecture</a>:
Read about the Nodes and Control Plane - Node communication.</p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/containers/">Containers</a> :
Read about
<a class="reference external" href="https://kubernetes.io/docs/concepts/containers/images/">images</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/">Workloads</a>: Read
about <a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/pods/">Pods</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">Pods
Lifecycle</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Init
Containers</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/controllers/">Workload
Resources</a>:
Read about
<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Jobs</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/services-networking/">Services, Load Balancing, and
Networking</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/storage/">Storage</a>: Read
about
<a class="reference external" href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent
Volumes</a>,
<a class="reference external" href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/">Ephemeral
Volumes</a></p></li>
</ul>
</section>
<section id="apache-mesos-dc-os-marathon">
<h4>Apache Mesos/DC-OS/Marathon<a class="headerlink" href="#apache-mesos-dc-os-marathon" title="Link to this heading"></a></h4>
<section id="apache-mesos">
<h5>Apache Mesos<a class="headerlink" href="#apache-mesos" title="Link to this heading"></a></h5>
<p>Apache Mesos abstracts CPU, memory, storage, and other compute resources
away from machines (physical or virtual), enabling fault-tolerant and
elastic distributed systems to be built and run effectively. The Mesos
kernel runs on every machine and provides applications (e.g., Hadoop,
Spark, Kafka, Elasticsearch) with API’s for resource management and
scheduling across entire datacenter and cloud environments.</p>
<p>Maybe start</p>
<ul class="simple">
<li><p>by watching <a class="reference external" href="https://www.youtube.com/watch?v=hTcZGODnyf0">Building and Running Distributed Systems using Apache
Mesos - Benjamin
Hindman</a></p></li>
<li><p>Reading about <a class="reference external" href="http://mesos.apache.org/documentation/latest/architecture/">Mesos
Architecture</a></p></li>
<li><p><a class="reference external" href="http://mesos.apache.org/documentation/latest/">Mesos
documentation</a></p></li>
</ul>
</section>
<section id="dc-os">
<h5>DC/OS<a class="headerlink" href="#dc-os" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://dcos.io/">DC/OS (the Distributed Cloud Operating System)</a> is
an open-source, distributed operating system based on the Apache Mesos
distributed systems kernel. DC/OS manages multiple machines in the cloud
or on-premises from a single interface; deploys containers, distributed
services, and legacy applications into those machines; and provides
networking, service discovery, and resource management to keep the
services running and communicating with each other.</p>
</section>
<section id="marathon">
<h5>Marathon<a class="headerlink" href="#marathon" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://mesosphere.github.io/marathon/">Marathon</a> is a
production-grade container orchestration platform for Mesosphere’s
<a class="reference external" href="https://dcos.io/">Datacenter Operating System (DC/OS)</a> and <a class="reference external" href="https://mesos.apache.org/">Apache
Mesos</a>.</p>
</section>
</section>
<section id="hashicorp-nomad">
<h4>Hashicorp Nomad<a class="headerlink" href="#hashicorp-nomad" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.nomadproject.io/">Nomad</a> is a simple and flexible
workload orchestrator to deploy and manage containers and
non-containerized applications across on-prem and clouds at scale.</p>
<p>Read <a class="reference external" href="https://learn.hashicorp.com/nomad">Explore Nomad</a></p>
</section>
<section id="kubernetes-hosted-solutions">
<h4>Kubernetes Hosted Solutions<a class="headerlink" href="#kubernetes-hosted-solutions" title="Link to this heading"></a></h4>
<p>There are many hosted solutions available for Kubernetes, including:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://aws.amazon.com/eks/">Amazon Elastic Kubernetes Service (Amazon
EKS)</a>: Offers a managed Kubernetes
service on AWS.</p></li>
<li><p><a class="reference external" href="https://azure.microsoft.com/en-us/services/kubernetes-service/">Azure Kubernetes Service
(AKS)</a>:
Offers managed Kubernetes clusters on Microsoft Azure.</p></li>
<li><p><a class="reference external" href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine
(GKE)</a>: Offers
managed Kubernetes clusters on Google Cloud Platform.</p></li>
<li><p><a class="reference external" href="https://www.ibm.com/cloud/container-service/">IBM Cloud Kubernetes
Service</a>: Fully
managed Kubernetes service at scale, providing continuous
availability and high availability, multi-zone, and multi-region
clusters.</p></li>
<li><p><a class="reference external" href="https://cloud.netapp.com/project-astra">NetApp Project Astra</a>
(fusion between NetApp and Stackpoint.io): Provides Kubernetes
infrastructure automation and management for multiple public clouds
optimized for stateful application data lifecycle management.</p></li>
<li><p><a class="reference external" href="https://www.oracle.com/cloud/compute/container-engine-kubernetes.html">Oracle Container Engine for Kubernetes
(OKE)</a>:
Enterprise-grade Kubernetes service offering highly available
clusters optimized to run on Oracle Cloud Infrastructure.</p></li>
<li><p><a class="reference external" href="https://www.openshift.com/products">Red Hat OpenShift</a>: Offers
managed Kubernetes clusters powered by Red Hat on various cloud
infrastructures such as AWS, GCP, Microsoft Azure, and IBM Cloud.</p></li>
<li><p><a class="reference external" href="https://tanzu.vmware.com/kubernetes-grid">VMware Tanzu Kubernetes Grid
(TKG)</a>: An
enterprise-grade multi-cloud Kubernetes service that runs both
on-premise in vSphere and in the cloud.</p></li>
</ul>
</section>
<section id="cloud-container-orchestration-services">
<h4>Cloud Container Orchestration Services<a class="headerlink" href="#cloud-container-orchestration-services" title="Link to this heading"></a></h4>
<section id="amazon-elastic-container-service">
<h5>Amazon Elastic Container Service<a class="headerlink" href="#amazon-elastic-container-service" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://aws.amazon.com/ecs/">Amazon Elastic Container Service (Amazon
ECS)</a> is a fully managed container
orchestration service that helps to easily deploy, manage and scale
containerized applications. It deeply integrates with the rest of the
AWS platform to provide a secure and easy-to-use solution for running
container workloads in the cloud and now on client infrastructure with
<a class="reference external" href="https://aws.amazon.com/ecs/anywhere/">Amazon ECS Anywhere</a>.</p>
</section>
<section id="azure-container-instances">
<h5>Azure Container Instances<a class="headerlink" href="#azure-container-instances" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://azure.microsoft.com/en-us/services/container-instances/">Azure Container
Instances</a>
provides developing apps fast without managing virtual machines or
having to learn new tools — it is just an application, in a container,
running in the cloud. They expose containers directly to the internet
through IP addresses and fully qualified domain names (FQDN).</p>
</section>
</section>
</section>
</section>
<section id="unikernels">
<h2>Unikernels<a class="headerlink" href="#unikernels" title="Link to this heading"></a></h2>
<p><a class="reference external" href="http://unikernel.org/">Unikernels website</a> defines <em>Unikernels are
specialized, single-address-space machine images constructed using
library operating systems.</em></p>
<p>Key benefits of unikernels:</p>
<ul class="simple">
<li><p>Improved security</p></li>
<li><p>Small footprints</p></li>
<li><p>Highly optimized</p></li>
<li><p>Fast Boot</p></li>
</ul>
<p>There are many implementations of
<a class="reference external" href="http://unikernel.org/projects/">unikernels</a>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://mirage.io/">Mirage OS</a>: MirageOS is a library operating
system that constructs unikernels for secure, high-performance
network applications across various cloud computing and mobile
platforms. Code can be developed on a standard OS such as Linux or
macOS, and then compiled into a fully standalone, specialized
unikernel that runs under a Xen or KVM hypervisor.</p></li>
<li><p><a class="reference external" href="http://osv.io/">OSv</a>: OSv is the versatile modular unikernel
designed to run unmodified Linux applications securely on micro-VMs
in the cloud.</p></li>
<li><p><a class="reference external" href="https://github.com/solo-io/unik">unik</a>: UniK is a tool for
compiling application sources into unikernels (lightweight bootable
disk images) and MicroVM rather than binaries. UniK runs and manages
instances of compiled images across a variety of cloud providers as
well as locally. UniK utilizes a simple docker-like command line
interface, making building unikernels and MicroVM as easy as building
containers.</p></li>
</ul>
</section>
<section id="microservices">
<h2>Microservices<a class="headerlink" href="#microservices" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Microservices">Wikipedia</a> defines
<em>Microservices are small, independent processes that communicate with
each other to form complex applications which utilize language-agnostic
APIs. These services are small building blocks, highly decoupled and
focused on doing a small task, facilitating a modular approach to
system-building. The microservices architectural style is becoming the
standard for building continuously deployed systems</em></p>
<p>Modern application development involves developing, deploying, and
managing a single application via a small set of services. Each service
runs its process and communicates with other services via lightweight
mechanisms like REST APIs. Each of these services is independently
deployed and managed.</p>
<p><a class="reference external" href="https://martinfowler.com/articles/microservices.html">Microservices</a>
is a good read, and the below diagram is taken from it.</p>
<figure class="align-default" id="id12">
<img alt="Monoliths and Microservices" src="../_images/monolith_microservices.png" />
<figcaption>
<p><span class="caption-text">Monoliths and Microservices</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="benefits">
<h3>Benefits<a class="headerlink" href="#benefits" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>As each service works and is deployed independently, Developers can
develop service components in any languages/technology as long as the
API endpoints return the expected output.</p></li>
<li><p>Each service can be updated or scaled independently without putting
the entire application offline.</p></li>
<li><p>Allows developers to reuse the functionality of the service code.</p></li>
<li><p>The microservice architecture enables continuous delivery.</p></li>
<li><p>Developers can deploy components across multiple servers or even
multiple data centers.</p></li>
<li><p>Microservices work very well with container orchestration tools
(Kubernetes, DC/OS, and Docker Swarm).</p></li>
</ul>
</section>
<section id="disadvantages">
<h3>Disadvantages<a class="headerlink" href="#disadvantages" title="Link to this heading"></a></h3>
<p>Choosing the right service size: As we need to refactor the monolith
application or create microservices from scratch, it is essential to
decide on a suitable functionality for a service. If there are too many
microservices, the complexity would be large.</p>
<ul class="simple">
<li><p>Deployment: Deploy of microservice requires a distributed environment
such as Kubernetes.</p></li>
<li><p>Testing: Requires end-to-end testing of a microservice.</p></li>
<li><p>Inter-service communication using either message passing/RPC needs to
be implemented effectively.</p></li>
<li><p>Managing databases: If multiple databases are used, they need to be
in sync.</p></li>
<li><p>Monitoring: Monitoring individual services and their health, usage
can be challenging and requires tools like Sysdig or Datadog.</p></li>
</ul>
</section>
</section>
<section id="software-defined-networking">
<h2>Software-Defined Networking<a class="headerlink" href="#software-defined-networking" title="Link to this heading"></a></h2>
<p>Software-Defined Networking (SDN) decouples the network control layer
from the traffic forwarding layer, allowing SDN to program the control
layer and create custom rules to meet the new networking requirements.</p>
<p>In networking, there are three distinctive planes:</p>
<ul class="simple">
<li><p>Data Plane: Responsible for handling data packets and apply actions
to them based on rules programmed into lookup tables.</p></li>
<li><p>Control Plane: Tasked with calculating and programming the actions
for the Data Plane. The Control plane defines the forwarding
decisions and implements services such as Quality of Service (QoS)
and VLANs.</p></li>
<li><p>Management Plane: Configure, monitor, and manage the network devices.</p></li>
</ul>
<p>Every network device has to perform three distinct activities:</p>
<ul class="simple">
<li><p>Ingress and egress packets: Process the packets based on the based on
forwarding tables.</p></li>
<li><p>Collect, process, and manage the network information to make the
forwarding decisions. Monitor and manage the network using the tools
available in the Management Plane, such as configuring the network
device and monitoring it with tools like SNMP (Simple Network
Management Protocol).</p></li>
</ul>
<p>In Software-Defined Networking, the Control Plane is decoupled from the
Data Plane. The Control Plane has a centralized view of the overall
network. It creates the forwarding tables based on which Data Plane
manages the network traffic. The control plane can be configured using
the API to configure the network, and then rules are communicated to the
Data Plane using a well-defined protocol like OpenFlow.</p>
<section id="networking-for-containers">
<h3>Networking for containers<a class="headerlink" href="#networking-for-containers" title="Link to this heading"></a></h3>
<p>Containers running on the same host and different hosts should be able
to communicate with each other. The host uses the <a class="reference external" href="https://lwn.net/Articles/580893/">network
namespace</a> to isolate the network
from one container to another on the system. Network namespaces can be
shared between containers as well.</p>
<section id="single-host">
<h4>Single Host<a class="headerlink" href="#single-host" title="Link to this heading"></a></h4>
<p>Each Linux container can be provided with a virtual network interface
with an assigned IP address using the virtual Ethernet (vEth) feature
with Linux bridging. Further, the container can be configured with a
unique worldwide routable IP address by using
<a class="reference external" href="https://docs.docker.com/network/macvlan/">Macvlan</a> and
<a class="reference external" href="https://www.kernel.org/doc/Documentation/networking/ipvlan.txt">IPVlan</a>.</p>
</section>
<section id="multi-host">
<h4>Multi Host<a class="headerlink" href="#multi-host" title="Link to this heading"></a></h4>
<p>The multi-host networking can be achieved by using the Overlay network
driver, which encapsulates the Layer 2 traffic to a higher layer. Such
implementations are provided by the Docker Overlay Driver, Flannel,
Weave, Calico.</p>
</section>
</section>
<section id="container-networking-standards">
<h3>Container Networking Standards<a class="headerlink" href="#container-networking-standards" title="Link to this heading"></a></h3>
<p>Two different standards have been proposed so far for container
networking:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/docker/libnetwork/blob/master/docs/design.md">Container Network Model
(CNM)</a>:
Mainly used by Docker. It is implemented using the libnetwork
project, which has the following utilizations:</p>
<ul>
<li><p>Null: NOOP implementation of the driver. It is used when no
networking is required.</p></li>
<li><p><a class="reference external" href="https://github.com/moby/libnetwork/blob/master/docs/bridge.md">Bridge</a>:
Provides a Linux-specific bridging implementation based in Linux
Bridge.</p></li>
<li><p><a class="reference external" href="https://github.com/moby/libnetwork/blob/master/docs/overlay.md">Overlay</a>:
Provides a multi-host communication over VXLAN.</p></li>
<li><p><a class="reference external" href="https://github.com/moby/libnetwork/blob/master/docs/remote.md">Remote</a>:
It does not provide a driver. Instead, it provides a means of
supporting drivers over a remote transport, by which developers
can write third-party drivers.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://github.com/containernetworking/cni">Container Networking Interface
(CNI)</a>: consists of a
specification and libraries for writing plugins to configure network
interfaces in Linux containers, along with several supported plugins.
CNI concerns itself only with network connectivity of containers and
removing allocated resources when the container is deleted. It is
used by projects like Kubernetes, OpenShift, and Cloud Foundry.</p></li>
</ul>
</section>
<section id="service-discovery">
<h3>Service Discovery<a class="headerlink" href="#service-discovery" title="Link to this heading"></a></h3>
<p>Service discovery is a mechanism by which processes and services can
find each other automatically and talk to each other. With respect to
containers, it is used to map a container name with its IP address to
access the container directly by its name without worrying about its
exact location (IP address), which may change during the life of the
container.</p>
<p>Service discovery is achieved in two steps:</p>
<ul class="simple">
<li><p>Registration: When a container starts, the container scheduler
registers the container name to the container IP mapping in a
key-value store such as etcd or Consul. And, if the container
restarts or stops, the scheduler updates the mapping accordingly.</p></li>
<li><p>Lookup: Services and applications use Lookup to retrieve the IP
address of a container so that they can connect to it. The DNS used
resolves the requests by looking at the entries in the key-value
store used for Registration.</p></li>
</ul>
</section>
<section id="docker-networking">
<h3>Docker Networking<a class="headerlink" href="#docker-networking" title="Link to this heading"></a></h3>
<section id="single-host-networking">
<h4>Single-Host Networking<a class="headerlink" href="#single-host-networking" title="Link to this heading"></a></h4>
<p>Networking setup on a single Docker host for its containers.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker network ls</span>
<span class="go">NETWORK ID     NAME      DRIVER    SCOPE</span>
<span class="go">acb6ec59eaed   bridge    bridge    local</span>
<span class="go">d2b0d4f71517   host      host      local</span>
<span class="go">15d5ceb9443e   none      null      local</span>
</pre></div>
</div>
<p>bridge, null, and host are different network drivers available on a
single Docker host.</p>
<section id="bridge-network">
<h5>Bridge Network<a class="headerlink" href="#bridge-network" title="Link to this heading"></a></h5>
<p>Linux enables the emulation of a software bridge on a Linux host to
forward traffic between two networks based on MAC (hardware address)
addresses. By default, Docker creates a <code class="docutils literal notranslate"><span class="pre">docker0</span></code> Linux bridge. Each
container running on a single host receives a unique IP address from
this bridge unless we specify some other network with the <code class="docutils literal notranslate"><span class="pre">--net=</span></code>
option. Docker uses Linux’s virtual Ethernet (vEth) feature to create
two virtual interfaces, with the interface on one end attached to the
container and the interface on the other end of the pair attached to the
<code class="docutils literal notranslate"><span class="pre">docker0</span></code> bridge.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500</span>
<span class="go">        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255</span>
<span class="go">        ether 02:42:2c:c3:36:7b  txqueuelen 0  (Ethernet)</span>
<span class="go">        RX packets 0  bytes 0 (0.0 B)</span>
<span class="go">        RX errors 0  dropped 0  overruns 0  frame 0</span>
<span class="go">        TX packets 0  bytes 0 (0.0 B)</span>
<span class="go">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span>
</pre></div>
</div>
<p>We can create a new container using the following command, then list its
IP address:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--name<span class="o">=</span>bb1<span class="w"> </span>busybox<span class="w"> </span>/bin/sh
<span class="go">/ # ip a</span>
<span class="go">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span>
<span class="go">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span>
<span class="go">    inet 127.0.0.1/8 scope host lo</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
<span class="go">1399: eth0@if1400: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue</span>
<span class="go">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff</span>
<span class="go">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
</pre></div>
</div>
<p>As we can see, the new container received its IP address from the
private IP address range <code class="docutils literal notranslate"><span class="pre">172.17.0.0/16</span></code>, which is catered by the
bridge network.</p>
<p>Inspecting Bridge Network</p>
<p>We can inspect the bridge network using</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker inspect bridge</span>
<span class="go">[</span>
<span class="go">    {</span>
<span class="go">        &quot;Name&quot;: &quot;bridge&quot;,</span>
<span class="go">        &quot;Id&quot;: &quot;acb6ec59eaedb2597092898acab922b53f45ba0243161917ab678bf8960bfc27&quot;,</span>
<span class="go">        &quot;Created&quot;: &quot;2020-04-01T17:25:26.004197458Z&quot;,</span>
<span class="go">        &quot;Scope&quot;: &quot;local&quot;,</span>
<span class="go">        &quot;Driver&quot;: &quot;bridge&quot;,</span>
<span class="go">        &quot;EnableIPv6&quot;: false,</span>
<span class="go">        &quot;IPAM&quot;: {</span>
<span class="go">            &quot;Driver&quot;: &quot;default&quot;,</span>
<span class="go">            &quot;Options&quot;: null,</span>
<span class="go">            &quot;Config&quot;: [</span>
<span class="go">                {</span>
<span class="go">                    &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;,</span>
<span class="go">                    &quot;Gateway&quot;: &quot;172.17.0.1&quot;</span>
<span class="go">                }</span>
<span class="go">            ]</span>
<span class="go">        },</span>
<span class="go">        &quot;Internal&quot;: false,</span>
<span class="go">        &quot;Attachable&quot;: false,</span>
<span class="go">        &quot;Ingress&quot;: false,</span>
<span class="go">        &quot;ConfigFrom&quot;: {</span>
<span class="go">            &quot;Network&quot;: &quot;&quot;</span>
<span class="go">        },</span>
<span class="go">        &quot;ConfigOnly&quot;: false,</span>
<span class="go">        &quot;Containers&quot;: {},</span>
<span class="go">        &quot;Options&quot;: {</span>
<span class="go">            &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;,</span>
<span class="go">            &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;,</span>
<span class="go">            &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;,</span>
<span class="go">            &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;,</span>
<span class="go">            &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;,</span>
<span class="go">            &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot;</span>
<span class="go">        },</span>
<span class="go">        &quot;Labels&quot;: {}</span>
<span class="go">    }</span>
<span class="go">]</span>
</pre></div>
</div>
<p>Currently, there are not any containers using the bridge; hence
<code class="docutils literal notranslate"><span class="pre">&quot;Containers&quot;:</span> <span class="pre">{},</span></code> is empty. Otherwise, it will contain the list of
containers using the bridge.</p>
</section>
<section id="creating-a-network-bridge">
<h5>Creating a network bridge<a class="headerlink" href="#creating-a-network-bridge" title="Link to this heading"></a></h5>
<p>A custom bridge network can be created using</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker network create --driver bridge my_bridge</span>
</pre></div>
</div>
<p>It creates a custom Linux bridge on the host system. To create a
container and have it use the newly created network, we have to start
the container with the <code class="docutils literal notranslate"><span class="pre">--net=my_bridge</span></code> option:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker container run --net=my_bridge -itd --name=c2 busybox</span>
</pre></div>
</div>
</section>
<section id="null-driver">
<h5>Null Driver<a class="headerlink" href="#null-driver" title="Link to this heading"></a></h5>
<p>As the name suggests, NULL means no networking. If we attach a container
to a null driver, then it would just get the loopback interface. It
would not be accessible from any other network.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker container run -it --name=c3 --net=none busybox /bin/sh</span>
<span class="go">/ # ip a</span>
<span class="go">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span>
<span class="go">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span>
<span class="go">    inet 127.0.0.1/8 scope host lo</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
</pre></div>
</div>
</section>
<section id="host-driver">
<h5>Host Driver<a class="headerlink" href="#host-driver" title="Link to this heading"></a></h5>
<p>Using the host driver, the host machine’s network namespace is shared
with a container. By doing so, the container would have full access to
the host’s network, which is not a recommended approach due to its
security implications. We can see below that running an <code class="docutils literal notranslate"><span class="pre">ifconfig</span></code>
command inside the container lists all the interfaces of the host
system:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker container run -it --name=c4 --net=host  busybox /bin/sh</span>
<span class="go">/ # ip a</span>
<span class="go">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000</span>
<span class="go">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span>
<span class="go">    inet 127.0.0.1/8 scope host lo</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
<span class="go">    inet6 ::1/128 scope host</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
<span class="go">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq qlen 1000</span>
<span class="go">    link/ether dc:a6:32:e5:8d:dc brd ff:ff:ff:ff:ff:ff</span>
<span class="go">    inet 192.168.1.207/24 brd 192.168.1.255 scope global dynamic eth0</span>
<span class="go">       valid_lft 63319sec preferred_lft 63319sec</span>
<span class="go">    inet6 2a00:23c7:8a86:7c00:dea6:32ff:fee5:8ddc/64 scope global dynamic noprefixroute flags 100</span>
<span class="go">       valid_lft 315359969sec preferred_lft 315359969sec</span>
<span class="go">    inet6 fdaa:bbcc:ddee:0:dea6:32ff:fee5:8ddc/64 scope global noprefixroute flags 100</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
<span class="go">    inet6 fe80::dea6:32ff:fee5:8ddc/64 scope link</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
<span class="go">3: wlan0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop qlen 1000</span>
<span class="go">    link/ether dc:a6:32:e5:8d:dd brd ff:ff:ff:ff:ff:ff</span>
<span class="go">5: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue</span>
<span class="go">    link/ether 02:42:2c:c3:36:7b brd ff:ff:ff:ff:ff:ff</span>
<span class="go">    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
<span class="go">    inet6 fe80::42:2cff:fec3:367b/64 scope link</span>
<span class="go">       valid_lft forever preferred_lft forever</span>
</pre></div>
</div>
<p>Sharing Network Namespaces Among Containers</p>
<p>Similar to the host, we can share network namespaces among containers.
As a result, two or more containers can share the same network stack and
reach each other through localhost.</p>
<p>Now, if we start a new container with the <code class="docutils literal notranslate"><span class="pre">--net=container:CONTAINER</span></code>
option, we can see that the second container has the same IP address.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker container run -it --name=c6 --net=container:c5 busybox /bin/sh</span>
</pre></div>
</div>
</section>
</section>
<section id="multi-host-networking">
<h4>Multi-Host Networking<a class="headerlink" href="#multi-host-networking" title="Link to this heading"></a></h4>
<p>Docker also supports multi-host networking, allowing containers from one
Docker host to communicate with containers from another Docker host..</p>
<ul class="simple">
<li><p><a class="reference external" href="(https://docs.docker.com/network/overlay/)">Docker Overlay
Driver</a>: libnetwork, a
built-in VXLAN-based overlay network driver allows docker to
encapsulate the container’s IP packet inside a host’s packet while
sending it over the wire. While receiving, Docker on the other host
decapsulates the whole packet and forwards the container’s packet to
the receiving container.</p></li>
<li><p><a class="reference external" href="https://docs.docker.com/network/macvlan/">Macvlan</a> driver: Docker
assigns a MAC (physical) address for each container and makes it
appear as a physical device on the network. As the containers appear
in the same physical network as the Docker host, they can be assigned
an IP from the network subnet as the host. As a result, direct
container-to-container communication between different hosts is
possible. Containers can also directly talk to hosts.</p></li>
</ul>
</section>
<section id="docker-plugins">
<h4>Docker Plugins<a class="headerlink" href="#docker-plugins" title="Link to this heading"></a></h4>
<p>Docker allows extending the functionality of Docker Engine by
implementing network, volume, and authentication
<a class="reference external" href="https://docs.docker.com/engine/extend/legacy_plugins/#use-docker-engine-plugins">plugins</a>.</p>
<p>Networking Plugins. For instance:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/contiv/netplugin">Contiv Networking Plugin</a>:
Provides infrastructure and security policies for multi-tenant
deployments.</p></li>
<li><p><a class="reference external" href="https://hub.docker.com/plugins/infoblox-ipam-plugin">Infoblox IPAM
Plugin</a>:
Infoblox ipam-plugin is a Docker libnetwork plugin that interfaces
with Infoblox to provide IP Address Management services.</p></li>
<li><p><a class="reference external" href="https://github.com/openstack/kuryr">Kuryr Network Plugin</a>: It is
a part of the OpenStack Kuryr project, which also implements
libnetwork’s remote driver API by utilizing Neutron, which is
OpenStack’s networking service.</p></li>
<li><p><a class="reference external" href="https://www.weave.works/docs/net/latest/install/plugin/plugin-how-it-works/">Weave Net Network
Plugin</a>:
Weave Net provides multi-host container networking for Docker. It
also provides service discovery and does not require any external
cluster store to save the networking configuration.</p></li>
</ul>
</section>
</section>
<section id="kubernetes-networking">
<h3>Kubernetes Networking<a class="headerlink" href="#kubernetes-networking" title="Link to this heading"></a></h3>
<p>Kubernetes assigns a unique IP address to each pod. Containers in a Pod
share the same network namespace and can refer to each other by
localhost. Containers in a Pod can expose unique ports and become
accessible through the same pod IP address. As each pod gets a unique
IP, Kubernetes assumes that Pods should be able to communicate with each
other, irrespective of the nodes they get scheduled on.</p>
<p>Kubernetes introduced the <a class="reference external" href="https://github.com/containernetworking/cni/blob/master/SPEC.md">Container Network Interface (CNI)
specification</a>
for container networking together with the following requirements that
need to be implemented by Kubernetes networking driver developers:</p>
<ul class="simple">
<li><p>All pods on a node can communicate with all pods on all nodes without
NAT</p></li>
<li><p>All nodes can communicate with all pods (and vice versa) without NAT</p></li>
<li><p>The IP that a pod sees itself as is the same IP that other pods see.</p></li>
</ul>
<p>Implementations of <a class="reference external" href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">Kubernetes
networking</a>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cilium.io/">Cilium</a>: Provides secure network connectivity
between application containers. It is L7/HTTP aware and can also
enforce network policies on L3-L7.</p></li>
<li><p><a class="reference external" href="https://github.com/coreos/flannel#flannel">Flannel</a>: Flannel uses
the overlay network, as we have seen with Docker, to meet the
Kubernetes networking requirements.</p></li>
<li><p><a class="reference external" href="https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.0/ncp-kubernetes/">NSX-T</a>:
NSX-T from VMware provides network virtualization for a multi-cloud
and multi-hypervisor environment. The NSX-T Container Plug-in (NCP)
provides integration between NSX-T and container orchestrators such
as Kubernetes.</p></li>
<li><p><a class="reference external" href="https://www.projectcalico.org/">Calico</a>: Calico uses the BGP
protocol to meet the Kubernetes networking requirements.</p>
<ul>
<li><p>Read <a class="reference external" href="https://docs.projectcalico.org/about/about-networking">About
networking</a></p></li>
<li><p>Read <a class="reference external" href="https://docs.projectcalico.org/about/about-kubernetes-networking">Kubernetes
Networking</a></p></li>
<li><p>Read <a class="reference external" href="https://docs.projectcalico.org/about/about-network-policy">Network
Policy</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.weave.works/oss/net/">Weave Net</a>: Weave Net, a simple
network for Kubernetes, may run as a CNI plugin or standalone. It
does not require additional configuration to run. The network
provides the one IP address per pod, as required and expected by
Kubernetes.</p></li>
</ul>
</section>
<section id="cloud-foundry-container-to-container-networking">
<h3>Cloud Foundry: Container to Container Networking<a class="headerlink" href="#cloud-foundry-container-to-container-networking" title="Link to this heading"></a></h3>
<p>By default,
<a class="reference external" href="https://docs.cloudfoundry.org/concepts/cf-routing-architecture.html">Gorouter</a>
routes the external and internal traffic to different Cloud Foundry (CF)
components.</p>
<p>The <a class="reference external" href="https://docs.cloudfoundry.org/concepts/understand-cf-networking.html">container-to-container
networking</a>
feature of CF enables application instances to communicate with each
other directly. However, when the container-to-container networking
feature is disabled, all application-to-application traffic must go
through the Gorouter.</p>
<p>Container-to-container networking is made possible by several components
of the CF architecture:</p>
<ul class="simple">
<li><p>Policy Server: A management node hosting a database of app traffic
policies.</p></li>
<li><p>Garden External Networker: Sets up networking for each app through
the CNI plugin, exposing apps to the outside by allowing incoming
traffic from Gorouter, TCP Router, and SSH Proxy.</p></li>
<li><p>Silk CNI Plugin: For IP management through a share VXLAN overlay
network that assigns each container a unique IP address. The overlay
network is not externally routable, and it prevents the
container-to-container traffic from escaping the overlay.</p></li>
<li><p>VXLAN Policy Agent: Enforces network policies between apps. When
creating routing rules for network policies, we should include the
source app, destination app, protocol, and ports without going
through the Gorouter, a load balancer, or a firewall.</p></li>
</ul>
</section>
</section>
<section id="software-defined-storage-and-storage-management">
<h2>Software-Defined Storage and Storage Management<a class="headerlink" href="#software-defined-storage-and-storage-management" title="Link to this heading"></a></h2>
<section id="software-defined-storage">
<h3>Software-Defined Storage<a class="headerlink" href="#software-defined-storage" title="Link to this heading"></a></h3>
<p>Software-Defined Storage (SDS) represents storage virtualization. The
underlying storage hardware is separated from the software that manages
and provisions it. The physical hardware from various sources is
combined and managed with software as a single storage pool. SDS
replaces static and inefficient storage solutions backed directly by
physical hardware with dynamic, agile, and automated solutions. SDS
provides resiliency features such as replication, erasure coding, and
snapshots of the pooled resources. Once the pooled storage is configured
in a storage cluster, SDS allows multiple access methods such as File,
Block, and Object.</p>
<p>Examples of Software-Defined Storage are: <a class="reference external" href="https://ceph.io/">Ceph</a>,
FreeNAS, <a class="reference external" href="https://www.gluster.org/">Gluster</a>,
<a class="reference external" href="https://linbit.com/">LINBIT</a>, <a class="reference external" href="https://min.io/">MinIO</a>,
<a class="reference external" href="https://nexenta.com/">Nexenta</a>, <a class="reference external" href="https://openebs.io/">OpenEBS</a>,
<a class="reference external" href="https://www.vmware.com/products/vsan.html">VMware vSAN</a>.</p>
<section id="ceph">
<h4>Ceph<a class="headerlink" href="#ceph" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://ceph.io/">Ceph</a> is a unified, distributed storage system
designed for excellent performance, reliability, and scalability. Ceph
supports applications with different storage interface needs. It
provides object, block, and file system storage in a single unified
storage cluster, making Ceph flexible, highly reliable, and easy to
manage.</p>
</section>
<section id="glusterfs">
<h4>GlusterFS<a class="headerlink" href="#glusterfs" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.gluster.org/">Gluster</a> is a free and open-source
software scalable network file system. Gluster can utilize common
off-the-shelf hardware to create large, distributed storage solutions
for media streaming, data analysis, and other data and
bandwidth-intensive tasks.</p>
</section>
</section>
<section id="storage-management-for-containers">
<h3>Storage Management for Containers<a class="headerlink" href="#storage-management-for-containers" title="Link to this heading"></a></h3>
<p>Containers are ephemeral, meaning that all data stored inside the
container’s file system would be lost when the container is deleted. It
is best practice to store data outside the container, which keeps the
data accessible even after the container is deleted.</p>
<p>In a multi-host or clustered environment, containers can be scheduled to
run on any host. We need to make sure the data volume required by the
container is available on the host on which the container is scheduled
to run.</p>
<section id="docker-storage-backends">
<h4>Docker Storage Backends<a class="headerlink" href="#docker-storage-backends" title="Link to this heading"></a></h4>
<p>Docker uses the copy-on-write mechanism when containers are started from
container images. The container image is protected from direct edits by
being saved on a read-only filesystem layer. All of the changes
performed by the container to the image filesystem are saved on a
writable filesystem layer of the container. Docker images and containers
are stored on the host system. We can choose the storage backend for
Docker storage, depending on our requirements. Docker supports the
following storage backends on Linux: AUFS (Another Union File System),
BtrFS, Device Mapper, Overlay, VFS (Virtual File System), ZFS.</p>
</section>
<section id="managing-data-in-docker">
<h4>Managing Data in Docker<a class="headerlink" href="#managing-data-in-docker" title="Link to this heading"></a></h4>
<p>Docker supports several options for storing files on a host system:</p>
<ul class="simple">
<li><p>Volumes: On Linux, volumes are stored under the
<code class="docutils literal notranslate"><span class="pre">/var/lib/docker/volumes</span></code> directory, and Docker directly manages
them. Volumes are the recommended method of storing persistent data
in Docker.</p></li>
<li><p>Bind Mounts: Allow Docker to mount any file or directory from the
host system into a container.</p></li>
<li><p>Tmpfs: Stored in the host’s memory only but not persisted on its
filesystem.</p></li>
<li><p>Named pipes (npipe): Commonly used for direct communication between a
container and the Docker host.</p></li>
</ul>
<p>Docker bypasses the Union filesystem by using the copy-on-write
mechanism in the cases of volumes and bind mounts. The writes happen
directly to the host directory.</p>
</section>
<section id="creating-a-container-with-volumes">
<h4>Creating a container with volumes<a class="headerlink" href="#creating-a-container-with-volumes" title="Link to this heading"></a></h4>
<p>In Docker, a container with a mounted volume can be created using either
the docker container run or the docker container create commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker container run -d --name web -v webvol:/webdata myapp:latest</span>
</pre></div>
</div>
<p>The above command would create a Docker volume inside the Docker working
directory <code class="docutils literal notranslate"><span class="pre">/var/lib/docker/volumes/webvol/_data</span></code> on the host system,
mounted on the container at the <code class="docutils literal notranslate"><span class="pre">/webdata</span></code> mount point. Exact mount
path can be found by using the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">inspect</span></code> command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker container inspect web</span>
</pre></div>
</div>
</section>
<section id="creating-a-named-volume">
<h4>Creating a named volume<a class="headerlink" href="#creating-a-named-volume" title="Link to this heading"></a></h4>
<p>We can give a specific name to a Docker volume and then use it for
different operations. To create a named volume, we can run the following
command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker volume create --name my-named-volume</span>
</pre></div>
</div>
<p>and then mount it.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker volume ls</span>
</pre></div>
</div>
</section>
<section id="mounting-a-host-directory-inside-the-container">
<h4>Mounting a host directory inside the container<a class="headerlink" href="#mounting-a-host-directory-inside-the-container" title="Link to this heading"></a></h4>
<p>In Docker, a container with a bind mount can be created using either the
<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">run</span></code> or the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">container</span> <span class="pre">create</span></code> commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker container run -d --name web -v /mnt/webvol:/webdata myapp:latest</span>
</pre></div>
</div>
<p>It mounts the host’s <code class="docutils literal notranslate"><span class="pre">/mnt/webvol</span></code> directory to <code class="docutils literal notranslate"><span class="pre">/webdata</span></code> mount
point on the container as it is being started.</p>
</section>
<section id="volume-plugins-for-docker">
<h4>Volume plugins for docker<a class="headerlink" href="#volume-plugins-for-docker" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://docs.docker.com/engine/extend/legacy_plugins/#volume-plugins">Volume
Plugins</a>
extend the functionality of the Docker Engine, allowing integrations
with third-party vendors storage solutions with the Docker ecosystem.
Examples: Azure File Storage, Blockbridge, DigitalOcean Block Storage,
Flocker, gce-docker, GlusterFS, Local Persists, NetApp (nDVP),
OpenStorage, REX-Ray, VMware vSphere Storage.</p>
<p>Volume plugins are helping while migrating a stateful container, like a
database, on a multi-host environment. In such an environment, we have
to make sure that the volume attached to a container is also migrated to
the host where the container is migrated or started afresh.</p>
<p>Flocker manages Docker containers and data volumes together by allowing
volumes to follow containers when they move between different hosts in
the cluster.</p>
</section>
</section>
<section id="volume-management-in-kubernetes">
<h3>Volume Management in Kubernetes<a class="headerlink" href="#volume-management-in-kubernetes" title="Link to this heading"></a></h3>
<p>Kubernetes uses volumes to attach external storage to containers managed
by Pods. A volume is essentially a directory backed by a storage medium.
The volume type determines the storage medium and its contents.</p>
<p>A volume in Kubernetes is linked to a Pod and shared among containers of
that pod. The volume has the same lifetime as the pod, but it outlives
the containers of that pod, meaning that data remains preserved across
container restarts. However, once the pod is deleted, the volume and all
its data are lost as well.</p>
<section id="volume-types">
<h4>Volume types<a class="headerlink" href="#volume-types" title="Link to this heading"></a></h4>
<p>A volume mounted inside a Pod is backed by an underlying volume type. A
volume type decides the properties of the volume, such as size and
content type. <a class="reference external" href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">Volume
types</a>
supported by Kubernetes are:</p>
<ul class="simple">
<li><p>awsElasticBlockStore: To mount an <a class="reference external" href="https://aws.amazon.com/ebs/">AWS
EBS</a> volume on containers of a Pod.</p></li>
<li><p>azureDisk: To mount an Azure Data Disk on containers of a Pod.</p></li>
<li><p>azureFile: To mount an Azure File Volume on containers of a Pod.</p></li>
<li><p>cephfs: To mount a CephFS volume on containers of a Pod.</p></li>
<li><p>configMap: To attach a decoupled object that includes configuration
data, scripts, and possibly entire filesystems, to containers of a
Pod.</p></li>
<li><p>emptyDir: An empty volume is created for the pod as soon as it is
scheduled on a worker node. The life of the volume is tightly coupled
with the pod. If the pod dies, the content of emptyDir is deleted
forever.</p></li>
<li><p>gcePersistentDisk: To mount a <a class="reference external" href="https://cloud.google.com/compute/docs/disks/">Google Compute Engine (GCE) persistent
disk</a> into a Pod.</p></li>
<li><p>glusterfs: To mount a <a class="reference external" href="https://www.gluster.org/">Glusterfs</a> volume
on containers of a Pod.</p></li>
<li><p>hostPath: To share a directory from the host with the containers of a
Pod. If the pod dies, the content of the volume is still available on
the host.</p></li>
<li><p>nfs: To mount an NFS share on containers of a Pod.</p></li>
<li><p>persistentVolumeClaim: To attach a persistent volume to a Pod.</p></li>
<li><p>rbd: To mount a <a class="reference external" href="https://docs.ceph.com/docs/master/rbd/">Rados Block
Device</a> volume on
containers of a Pod.</p></li>
<li><p>secret: To attach sensitive information such as passwords, keys,
certificates, or tokens to containers in a Pod.</p></li>
<li><p>vsphereVolume: To mount a vSphere VMDK volume on containers of a Pod.</p></li>
</ul>
</section>
<section id="persistant-volumes">
<h4>Persistant Volumes<a class="headerlink" href="#persistant-volumes" title="Link to this heading"></a></h4>
<p>Persistent volume subsystem provides APIs to manage and consume the
storage. To manage the volume, it uses the <a class="reference external" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolume
(PV)</a>
resource type and to consume it; it uses the <a class="reference external" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim
(PVC)</a>
resource type.</p>
<p>Persistent volumes can be provisioned statically or dynamically. For
dynamic provisioning of PVs, Kubernetes uses the StorageClass resource,
which contains pre-defined provisioners and parameters for the PV
creation. With PersistentVolumeClaim (PVC), a user sends the requests
for dynamic PV creation, which gets wired to the StorageClass resource.</p>
<p><a class="reference external" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Volume
types</a>
that support managing storage using PersistentVolume are:
GCEPersistentDisk, AWSElasticBlockStore, AzureFile, AzureDisk, NFS,
iSCSI, RBD, CephFS, GlusterFS, VsphereVolume, StorageOS.</p>
</section>
<section id="persistent-volumes-claim">
<h4>Persistent Volumes Claim<a class="headerlink" href="#persistent-volumes-claim" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request for PV resources based on size, access modes, and volume type.</p></li>
<li><p>Once a suitable PV is found, it is bound to PVC. After a successful bind, the PVC can be used in a Pod to allow the container’s access to the PV.</p></li>
<li><p>Once users complete their tasks and the pod is deleted, the PVC may be detached from the PV, releasing it for possible future use. However, keep in mind that the PVC may be detached from the PV once all
the Pods using the same PVC have completed their activities and have been deleted. Once released, the PV can be either deleted, retained, or recycled for future usage, all based on the reclaim policy the user has configured on the PV.</p></li>
</ul>
</section>
<section id="container-storage-interface-csi">
<h4>Container Storage Interface (CSI)<a class="headerlink" href="#container-storage-interface-csi" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Previously, container orchestrators like Kubernetes, Mesos, Docker, and Cloud Foundry have had their specific methods of managing external storage volumes making it difficult for storage vendors, to
manage different volume plugins for different orchestrators.</p></li>
<li><p>Storage vendors and community members from different orchestrators have worked together to standardize the volume interface and created <a class="reference external" href="https://github.com/container-storage-interface/spec">Container Storage Interface (CSI)</a>
such that the same volume plugin would work with different containers orchestrators out of the box.</p></li>
<li><p>The role of CSI is to maintaining the <a class="reference external" href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI specification</a> and the <a class="reference external" href="https://github.com/container-storage-interface/spec/blob/master/csi.proto">protobuf</a>.</p></li>
<li><p>The goal of the CSI specification is to define APIs for dynamic provisioning, attaching, mounting, consumption, and snapshot management of storage volumes. In addition, it defines the plugin configuration steps to be taken by the container orchestrator together with deployment configuration options.</p></li>
</ul>
</section>
<section id="cloud-foundry-volume-service">
<h4>Cloud Foundry Volume Service<a class="headerlink" href="#cloud-foundry-volume-service" title="Link to this heading"></a></h4>
<p>On Cloud Foundry, applications connect to other services via a service
marketplace. Each service has a service broker, which encapsulates the
logic for creating, managing, and binding services to applications. With
volume services, the volume service broker allows Cloud Foundry
applications to attach external storage.</p>
<p>With the <code class="docutils literal notranslate"><span class="pre">volume</span> <span class="pre">bind</span></code> command, the service broker issues a volume
mount instruction that instructs the Diego scheduler to schedule the app
instances on cells with the appropriate volume driver. In the backend,
the volume driver gets attached to the device. Cells then mount the
device into the container and start the app instance.</p>
<p>Following are some examples of CF Volume Services:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/cloudfoundry/nfs-volume-release">nfs-volume-release</a>:
This allows for easy mounting of external Network File System (NFS)
shares for Cloud Foundry applications.</p></li>
<li><p><a class="reference external" href="https://github.com/cloudfoundry/smb-volume-release">smb-volume-release</a>:
This allows for easy mounting of external Server Message Block (SMB)
shares for Cloud Foundry applications.</p></li>
</ul>
</section>
</section>
</section>
<section id="devops-and-ci-cd">
<h2>DevOps and CI/CD<a class="headerlink" href="#devops-and-ci-cd" title="Link to this heading"></a></h2>
<p>The collaborative work between Developers and Operations is referred to
as DevOps. DevOps is more of a mindset, a way of thinking, versus a set
of processes implemented in a specific way.</p>
<p>Besides Continuous Integration (CI), DevOps also enables Continuous
Deployment (CD), which can be seen as the next step of CI. In CD, we
deploy the entire application/software automatically, provided that all
the tests’ results and conditions have met the expectations.</p>
<section id="why-ci">
<h3>Why CI<a class="headerlink" href="#why-ci" title="Link to this heading"></a></h3>
<p>When code is developed in an organisation, multiple developers can be
contributing to the code and multiple challenges can occur. The
following factors need to be taken into consideration while developing
and testing the code:</p>
<ul class="simple">
<li><p>Can overlapping sets of changes be applied simultaneously, or do they
conflict (a good revision control system such as git can handle most
of this work, but it still often requires human intervention).</p></li>
<li><p>When all changes are applied, does the project even compile? For
example, one patch set might remove a header file that another one
needs. This does not get picked up by the revision control system.</p></li>
<li><p>Does it work on all possible targets? That might mean different
hardware (say x86 vs. ARM) or different operating systems (say Linux
vs. Solaris or Windows) or different library, language, or utility
versions.</p></li>
<li><p>What does working mean? Are there non-trivial test suites that can
exercise a representative workload enough to give confidence things
are fine?</p></li>
</ul>
<p>Continuous Integration techniques ensure that testing is so frequent
that any problems cannot persist for long and that distributed
developers stay on the same page. Projects absorb changes rapidly and in
real time (usually multiple times per day) and run automated tests to
make sure things are in harmony.</p>
</section>
<section id="definitions">
<h3>Definitions<a class="headerlink" href="#definitions" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Continous Integration: Changes are to be merged into the main branch
as often as possible. Automated builds are run on as many variations
of software and hardware as possible; conflicts are resolved as soon
as they arise.</p></li>
<li><p>Continous Delivery: The release process is automated and projects are
ready to be delivered to consumers of the build. Thorough testing is
done on all relevant platforms.</p></li>
<li><p>Continous Deployment: The product is released to customers, once
again in an automated fashion.</p></li>
</ul>
<p>The time gap between these steps is meant to be as close to zero as
possible. In a perfect world, developer changes can reach end user
customers the same day or even in minutes. These terms can be defined
somewhat differently; for example, Continuous Integration can be
considered to include both delivery and deployment.</p>
<p>Costs:</p>
<ul class="simple">
<li><p>Changes have to be merged very often, probably at least once a day,
putting a possible strain on developers.</p></li>
<li><p>The repository must be monitored by a continuous integration server,
which runs scripted automation tests every time contributions are
made. Staff has to be allocated to do this.</p></li>
<li><p>Scripts and other tools have to be run to perform automated tests,
report their results, and take appropriate actions. It can be a lot
of work to prepare this infrastructure.</p></li>
</ul>
<p>Benefits:</p>
<ul class="simple">
<li><p>Developers do not go down the wrong path and compound fixable
mistakes or get in each other’s way. In the end, this saves time.</p></li>
<li><p>The build steps are fully automated, all the work has been done
upfront.</p></li>
<li><p>Regressions (bugs which break the working product) may be minimized.
Releases should have fewer bugs.</p></li>
</ul>
</section>
<section id="ci-cd-tools">
<h3>CI/CD Tools<a class="headerlink" href="#ci-cd-tools" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://stackify.com/top-continuous-integration-tools/">Top Continuous Integration Tools: 51 Tools to Streamline Your
Development Process, Boost Quality, and Enhance
Accuracy</a>
provides a good summary of CI tools. Some of the main tools are Jenkins,
Travis CI, TeamCity, GoCD, GitLab CI, Bamboo, Codeship, CircleCI.</p>
<p>Some of the software used in the CI/CD domain are Jenkins, Travis CI,
Shippable, and Concourse</p>
<section id="jenkins">
<h4>Jenkins<a class="headerlink" href="#jenkins" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.jenkins.io/">Jenkins</a> is a open source automation
system, part of the <a class="reference external" href="https://cd.foundation/">CD Foundation</a>. It can
provide Continuous Integration (CI) and Continuous Deployment (CD), and
it is written in Java. Jenkins can build Freestyle, Apache Ant, and
Apache Maven-based projects. We can also extend the functionality of
Jenkins, using
<a class="reference external" href="https://plugins.jenkins.io/ui/search/?query=">plugins</a> such as
Source Code Management, Slave Launchers, Build tools, and External
tools/site integration.</p>
<p>Jenkins also has the functionality to build a pipeline, which allows us
to define an entire application lifecycle. A pipeline is most useful for
performing Continuous Deployment (CD).</p>
<p>Start reading</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.jenkins.io/doc/">Jenkins User Documentation</a></p></li>
<li><p><a class="reference external" href="https://www.jenkins.io/doc/book/pipeline/">Pipeline</a> is a suite
of plugins which supports implementing and integrating continuous
delivery pipelines into Jenkins.</p></li>
</ul>
</section>
<section id="travis-ci">
<h4>Travis CI<a class="headerlink" href="#travis-ci" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://travis-ci.com/">Travis CI</a> is a hosted, distributed CI
solution for projects hosted on GitHub, Bitbucket, and more.</p>
<p>To run the test with CI, first we have to link our GitHub account with
Travis and select the project (repository) for which we want to run the
test. In the project’s repository, we have to create a <code class="docutils literal notranslate"><span class="pre">.travis.yml</span></code>
file, which defines how our build should be executed step-by-step.</p>
<p>A typical build with Travis consists of two steps:</p>
<ul class="simple">
<li><p>install: to install any dependency or pre-requisite</p></li>
<li><p>script: to run the build script.</p></li>
</ul>
<p>There are other optional steps, including the deployment steps. The
following are all the build options one can put in a <code class="docutils literal notranslate"><span class="pre">.travis.yml</span></code>
file: before_install, install, before_script, script, before_cache,
after_success or after_failure, before_deploy, deploy, after_deploy,
after_script.</p>
<p>Travis CI supports various databases, such as MYSQL, RIAK, and
memcached. We can also use Docker during the build process. Travis CI
supports most
<a class="reference external" href="https://docs.travis-ci.com/user/language-specific/">languages</a>.</p>
<p>After running the test, we can deploy the application in many <a class="reference external" href="https://docs.travis-ci.com/user/deployment">supported
cloud providers</a>, such as
Heroku, AWS Codedeploy, Cloud Foundry, and OpenShift.</p>
</section>
<section id="shippable">
<h4>Shippable<a class="headerlink" href="#shippable" title="Link to this heading"></a></h4>
<p><a class="reference external" href="http://docs.shippable.com/">Shippable</a> is a DevOps Assembly Lines
Platform that helps developers and DevOps teams achieve CI/CD and make
software releases frequent, predictable, and error-free. It does this by
connecting all your DevOps tools and activities into a event-driven,
stateful workflow.</p>
</section>
<section id="consource">
<h4>Consource<a class="headerlink" href="#consource" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://concourse-ci.org/">Concourse</a> is an open source CI/CD system
written in the Go language.</p>
</section>
</section>
<section id="ci-cd-kubernetes-tools">
<h3>CI/CD Kubernetes Tools<a class="headerlink" href="#ci-cd-kubernetes-tools" title="Link to this heading"></a></h3>
<p>Tools that help us with CI/CD, which integrate well with Kubernetes for
Cloud Native applications:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://helm.sh/">Helm</a> : Package manager for Kubernetes. Helm
packages Kubernetes applications into Charts, with all the artifacts,
objects, and dependencies an entire application needs in order to
successfully be deployed in a Kubernetes cluster. Using Helm Charts,
which are stored in repositories, we can share, install, upgrade, or
rollback an application that was built to run on Kubernetes.</p></li>
<li><p>Draft: Developer tool for cloud-native applications running on
Kubernetes, and it allows for an application to be containerized and
deployed on Kubernetes.</p></li>
<li><p><a class="reference external" href="https://skaffold.dev/">Skaffold</a>: Tool that helps us build, push,
and deploy code to the Kubernetes cluster. It supports Helm, and it
also provides building blocks and describes customizations for a
CI/CD pipeline.</p></li>
<li><p><a class="reference external" href="https://argoproj.github.io/">Argo</a>: Container-native workflow
engine for Kubernetes. Its use cases include running Machine
Learning, Data Processing, and CI/CD tasks on Kubernetes.</p></li>
<li><p><a class="reference external" href="https://jenkins-x.io/">Jenkins X</a>: Tool for CI/CD that can be
used on Kubernetes. Jenkins X leverages Docker, Draft, and Helm to
deploy a Jenkins CI/CD pipeline directly on Kubernetes, by
simplifying and automating full CI/CD pipelines. In addition, Jenkins
X automates the preview of pull requests for fast feedback before
changes are merged, and then it automates the environment management
and the promotion of new application versions between different
environments.</p></li>
<li><p><a class="reference external" href="https://www.spinnaker.io/">Spinnaker</a>: Open source multi-cloud
continuous delivery platform from Netflix for releasing software
changes with high velocity. It supports all the major cloud providers
like Amazon Web Services, Microsoft Azure, Google Cloud Platform, and
OpenStack. It supports Kubernetes natively.</p></li>
</ul>
<p>Other Information:</p>
<ul class="simple">
<li><p>The Linux kernel development community employs a very robust
continuous integration project called
<a class="reference external" href="https://kernelci.org/">kernelci</a>.</p></li>
</ul>
</section>
</section>
<section id="tools-for-cloud-infrastructure">
<h2>Tools for Cloud Infrastructure<a class="headerlink" href="#tools-for-cloud-infrastructure" title="Link to this heading"></a></h2>
<section id="configuration-management">
<h3>Configuration Management<a class="headerlink" href="#configuration-management" title="Link to this heading"></a></h3>
<p>As an administrator, we want to manage and automate numerous systems
(both bare-metal and virtual) in different environments like
Development, QA, and Production. Infrastructure as Code allows having a
consistent and desired state of systems and software installed on them.
Configuration Management tools allow us to define the desired state of
the systems in an automated way.</p>
<section id="ansible">
<h4>Ansible<a class="headerlink" href="#ansible" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.ansible.com/">Ansible</a> is an easy-to-use, open-source
Configuration Management (CM) tool. It is an agentless tool that works
through SSH. In addition, Ansible can automate infrastructure
provisioning (on-premise or cloud), application deployment, and
orchestration.
<a class="reference external" href="https://docs.ansible.com/ansible/latest/user_guide/playbooks.html">Playbooks</a>
are Ansible’s configuration, deployment, and orchestration language.</p>
<p>The Ansible management node connects to the nodes listed in the
inventory file. It runs the tasks included in the playbook.
Administrators can install a management node on any Unix-based system
like Linux or Mac OS X. It can manage any node which supports SSH and
Python.</p>
<p><a class="reference external" href="https://galaxy.ansible.com/">Ansible Galaxy</a> is a free site for
finding, downloading, and sharing community-developed Ansible roles.</p>
</section>
<section id="puppet">
<h4>Puppet<a class="headerlink" href="#puppet" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://puppet.com/">Puppet</a> is an open-source configuration
management tool. It mostly uses the agent/master (client/server) model
to configure the systems. The agent is referred to as the Puppet Agent,
and the master is referred to as the Puppet Master. The Puppet Agent can
also work locally and is then referred to as Puppet Apply.</p>
<p>Puppet also provides <a class="reference external" href="https://puppet.com/products/puppet-enterprise/">Puppet
Enterprise</a> and
provides services and training for this product.</p>
<section id="puppet-agent">
<h5>Puppet Agent<a class="headerlink" href="#puppet-agent" title="Link to this heading"></a></h5>
<p>The Puppet Agent needs to be installed on each system we want to
manage/configure with Puppet. Each agent is responsible for:</p>
<ul class="simple">
<li><p>Connecting securely to the Puppet Master to get the series of
instructions in a file referred to as the Catalog File.</p></li>
<li><p>Performing operations from the Catalog File to get to the desired
state.</p></li>
<li><p>Sending back the status to the Puppet Master.</p></li>
</ul>
<p>Puppet Agent can be installed on the following platforms: Linux,
Windows, Mac OSX.</p>
</section>
<section id="puppet-master">
<h5>Puppet Master<a class="headerlink" href="#puppet-master" title="Link to this heading"></a></h5>
<p>Puppet Master can be installed only on Unix-based systems. It is
responsible for:</p>
<ul class="simple">
<li><p>Compiling the Catalog File for hosts based on system, configuration,
manifest file, etc.</p></li>
<li><p>Sending the Catalog File to agents when they query the master.</p></li>
<li><p>Storing information about the entire environment, such as host
information, metadata such as authentication keys.</p></li>
<li><p>Gathering reports from each agent and then preparing the overall
report.</p></li>
</ul>
</section>
<section id="catalog-file">
<h5>Catalog File<a class="headerlink" href="#catalog-file" title="Link to this heading"></a></h5>
<p>Puppet prepares a Catalog File based on the manifest file. A manifest
file is created using the Puppet Code.</p>
<p>Puppet defines
<a class="reference external" href="https://puppet.com/docs/puppet/latest/resource_types.html">resources</a>
on a system as Type, which can be a file, user, package, service, etc.
They are well-documented in their documentation.</p>
<p>After processing the manifest file, the Puppet Master prepares the
Catalog File based on the target platform.</p>
</section>
<section id="puppet-tools">
<h5>Puppet Tools<a class="headerlink" href="#puppet-tools" title="Link to this heading"></a></h5>
<p>Puppet also has nice tooling around it, like:</p>
<ul class="simple">
<li><p>Centralized reporting through
<a class="reference external" href="https://puppet.com/docs/puppetdb/latest/index.html">PuppetDB</a>
helps us generate reports and search a system, etc.</p></li>
<li><p>Live system management.</p></li>
<li><p><a class="reference external" href="https://forge.puppet.com/">Puppet Forge</a> has ready-to-use modules
for manifest files from the community.</p></li>
</ul>
</section>
</section>
<section id="chef">
<h4>Chef<a class="headerlink" href="#chef" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.chef.io/">Chef</a> uses the client/server model to perform
configuration management. The <a class="reference external" href="https://docs.chef.io/chef_client_overview/">Chef
client</a> is installed on
each host which we want to manage. The server is referred to as <a class="reference external" href="https://docs.chef.io/server_overview/">Chef
Server</a>. Additionally, there
is another component called <a class="reference external" href="https://docs.chef.io/workstation/">Chef
Workstation</a>, which is used to:</p>
<ul class="simple">
<li><p>Develop <a class="reference external" href="https://docs.chef.io/chef_overview/#cookbooks">cookbooks</a>
and <a class="reference external" href="https://docs.chef.io/recipes/">recipes</a>.</p></li>
<li><p>Synchronize <a class="reference external" href="https://docs.chef.io/chef_repo/">chef-repo</a> with the
version control system.</p></li>
<li><p>Run command-line tools.</p></li>
<li><p>Configure <a class="reference external" href="https://docs.chef.io/policy/">policy</a>,
<a class="reference external" href="https://docs.chef.io/roles/">roles</a>, etc.</p></li>
<li><p>Interact with nodes to perform a one-off configuration.</p></li>
</ul>
</section>
<section id="salt-stack">
<h4>Salt Stack<a class="headerlink" href="#salt-stack" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://saltproject.io/">Salt</a> is an open-source configuration
management system built on top of a remote execution framework. It can
be used in a client/server model or agentless model as well. In a
client/server model, the server sends commands and configurations to all
the clients in a parallel manner, which the clients run, returning the
status. In the agentless mode, the server connects with remote systems
via SSH.</p>
<ul class="simple">
<li><p>Each client is referred to as a Salt minion, receiving configuration
commands from the Master, and reporting back results:</p></li>
<li><p>A server is referred to as a Salt master. In a default setup, the
Salt master and minions communicate over a high-speed data bus,
<a class="reference external" href="http://zeromq.org/">ZeroMQ</a>, which requires an agent to be
installed on each minion.</p></li>
<li><p>Salt also supports an agentless setup using SSH for secure and
encrypted communication between the master and the managed systems.</p></li>
</ul>
</section>
</section>
<section id="build-and-release">
<h3>Build and Release<a class="headerlink" href="#build-and-release" title="Link to this heading"></a></h3>
<p>Infrastructure as a Code helps us create a near production-like
environment for development, staging, etc. Now, suppose we want to
create the environment on multiple cloud providers. In that case, we can
use Infrastructure management tools such as Terraform, CloudFormation,
and BOSH.</p>
<section id="terraform">
<h4>Terraform<a class="headerlink" href="#terraform" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.terraform.io/">Terraform</a> is a tool that allows us to
define the infrastructure as code. This helps us deploy the same
infrastructure on Virtual Machines, bare metal, or cloud. It helps us
treat the infrastructure as software. The configuration files can be
written in <a class="reference external" href="https://github.com/hashicorp/hcl">HCL</a> (HashiCorp
Configuration Language).</p>
<section id="terraform-providers">
<h5>Terraform Providers<a class="headerlink" href="#terraform-providers" title="Link to this heading"></a></h5>
<p>Physical machines, VMs, network switches, or containers are treated as
resources, which providers expose. A provider is responsible for
understanding API interactions and exposing resources, making Terraform
agnostic to the underlying platforms.</p>
<p>Terraform has providers in different stacks:</p>
<ul class="simple">
<li><p>IaaS: AWS, DigitalOcean, GCP, OpenStack, Azure, Alibaba Cloud, KVM.</p></li>
<li><p>PaaS: Heroku, Cloud Foundry, etc.</p></li>
<li><p>SaaS: Atlas, DNSimple, etc.</p></li>
</ul>
</section>
</section>
<section id="cloudformation">
<h4>CloudFormation<a class="headerlink" href="#cloudformation" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://aws.amazon.com/cloudformation/">CloudFormation</a> is a tool
that allows us to define our infrastructure as code on Amazon AWS. The
configuration files can be written in YAML or JSON format. At the same
time, CloudFormation can also be used from a web console or the command
line.</p>
</section>
<section id="bosh">
<h4>BOSH<a class="headerlink" href="#bosh" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://bosh.io/">BOSH</a> is an open-source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.</p></li>
<li><p>BOSH supports multiple Infrastructure as a Service (IaaS) providers. BOSH creates VMs on top of IaaS, configures them to suit the requirements, and then deploys the applications on them. Supported IaaS providers for BOSH are Amazon Web Services EC2, OpenStack, VMware vSphere, vCloud Director.</p></li>
<li><p>With the Cloud Provider Interface (CPI), BOSH supports additional IaaS providers such as <a class="reference external" href="https://github.com/cloudfoundry/bosh-google-cpi-release">Google Compute Engine</a> and <a class="reference external" href="https://github.com/orange-cloudfoundry/bosh-cloudstack-cpi-release">Apache CloudStack</a>.</p></li>
</ul>
</section>
</section>
<section id="key-value-pair-store">
<h3>Key-Value Pair Store<a class="headerlink" href="#key-value-pair-store" title="Link to this heading"></a></h3>
<p>Key-Value Pair Storage provides the functionality to store or retrieve
the value of a key. Most of the Key-Value stores provide REST APIs to
support operations like GET, PUT, and DELETE, which help with operations
over HTTP.</p>
<section id="etcd">
<h4>etcd<a class="headerlink" href="#etcd" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://etcd.io/">etcd</a> is a strongly consistent, distributed
key-value store that provides a reliable way to store data that needs to
be accessed by a distributed system or cluster of machines. It
gracefully handles leader elections during network partitions and can
tolerate machine failure, even in the leader node. It allows users or
services to watch the value of a key and then perform certain operations
due to any change in that particular value.</p>
<section id="etcd-use-cases">
<h5>etcd use-cases<a class="headerlink" href="#etcd-use-cases" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>Store connections, configuration, cluster bootstrapping keys, and
other settings.</p></li>
<li><p>Service Discovery in conjunction with tools like skyDNS.</p></li>
<li><p>Metadata and configuration data for service discovery.</p></li>
<li><p>Container management.</p></li>
</ul>
</section>
</section>
<section id="consul">
<h4>Consul<a class="headerlink" href="#consul" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.consul.io/">Consul</a> is a distributed, highly-available
system used for service discovery and configuration. Other than
providing a distributed key-value store, it also provides features like:</p>
<ul class="simple">
<li><p>Service discovery in conjunction with DNS or HTTP</p></li>
<li><p>Health checks for services and nodes</p></li>
<li><p>Multi-datacenter support.</p></li>
</ul>
<section id="consul-use-cases">
<h5>Consul use-cases<a class="headerlink" href="#consul-use-cases" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>Store connections, configuration, and other settings.</p></li>
<li><p>Service discovery and health checks in conjunction with DNS or HTTP.</p></li>
<li><p>Network infrastructure automation with dynamic load balancing while
reducing downtime and outages.</p></li>
<li><p>Multi-platform secure service-to-service communication.</p></li>
</ul>
</section>
</section>
<section id="zookeeper">
<h4>ZooKeeper<a class="headerlink" href="#zookeeper" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://zookeeper.apache.org/">ZooKeeper</a> is a centralized service
for maintaining configuration information, providing distributed
synchronization together with group services for distributed
applications.</p>
<p>ZooKeeper aims to provide a simple interface to a centralized
coordination service that is also distributed and highly reliable. It
implements Consensus, group management, and presence protocols on behalf
of applications</p>
<section id="zookeeper-use-cases">
<h5>Zookeeper use-cases<a class="headerlink" href="#zookeeper-use-cases" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>Implement node coordination in a clustered environment.</p></li>
<li><p>To manage cloud node memberships while coordinating distributed jobs.</p></li>
<li><p>As a backing store for distributed and scalable data structures.</p></li>
<li><p>Election of a High Availability master.</p></li>
<li><p>As light-weight failover and load balancing manager</p></li>
</ul>
</section>
</section>
</section>
<section id="container-image-building">
<h3>Container Image Building<a class="headerlink" href="#container-image-building" title="Link to this heading"></a></h3>
<p>We often need to create an image in an automated fashion from which the
service can be started. We can create Docker container images and VM
images for different cloud platforms using Packer.</p>
<section id="docker-1">
<span id="id2"></span><h4>Docker<a class="headerlink" href="#docker-1" title="Link to this heading"></a></h4>
<p>Docker read instructions from a
<a class="reference external" href="https://docs.docker.com/engine/reference/builder/">Dockerfile</a> and
generates the requested image. Internally, it creates a container after
each instruction and then commits it to persistent storage. FROM, RUN,
EXPOSE, and CMD are reserved instructions and are followed by arguments.
Sample Dockerfile:</p>
<div class="highlight-docker notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">fedora</span>
<span class="k">RUN</span><span class="w"> </span>dnf<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>dnf<span class="w"> </span>clean<span class="w"> </span>all
<span class="k">RUN</span><span class="w"> </span>dnf<span class="w"> </span>-y<span class="w"> </span>install<span class="w"> </span>nginx<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>dnf<span class="w"> </span>clean<span class="w"> </span>all
<span class="k">RUN</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;daemon off;&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>/etc/nginx/nginx.conf
<span class="k">RUN</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;nginx on Fedora&quot;</span><span class="w"> </span>&gt;<span class="w"> </span>/usr/share/nginx/html/index.html

<span class="k">EXPOSE</span><span class="w"> </span><span class="s">80</span>

<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="s2">&quot;/usr/sbin/nginx&quot;</span><span class="w"> </span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Dockerfiles start from a parent image or a base image, specified with
the FROM instruction.</p></li>
<li><p>A parent image is an image used as a reference, modified by
subsequent Dockerfile instructions.</p></li>
<li><p>Users can build parent images directly out of working machines or
with tools such as
<a class="reference external" href="https://wiki.debian.org/Debootstrap">Debootstrap</a> or
<a class="reference external" href="https://github.com/libguestfs/supermin">supermin</a>. The <a class="reference external" href="https://github.com/docker/docker/tree/master/contrib">Docker
GitHub source code
repository</a>
also has helper scripts to create base images.</p></li>
<li><p>A base image has FROM scratch in the Dockerfile.</p></li>
</ul>
<section id="multi-stage-dockerfile">
<h5>Multi-stage Dockerfile<a class="headerlink" href="#multi-stage-dockerfile" title="Link to this heading"></a></h5>
<p>A <a class="reference external" href="https://docs.docker.com/develop/develop-images/multistage-build/">multi-stage
build</a>
helps optimize the Dockerfiles and minimize the size of a container
image. Through a multi-stage build, we create a new Docker image in
every stage. Every stage can copy files from images created either in
earlier stages or from already available images.</p>
<p>For example,</p>
<ul class="simple">
<li><p>In the first stage, we can copy the source code, compile it to create
a binary, and then</p></li>
<li><p>In the second stage, we can copy just the binary to a resulting
image.</p></li>
</ul>
<p>Before multi-stage builds, we would need to create multiple Dockerfiles
to achieve similar results.</p>
<p>Let’s take a look at the following Dockerfile:</p>
<p>stage - 1</p>
<div class="highlight-docker notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">ubuntu</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">buildstep</span>
<span class="k">RUN</span><span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>build-essential<span class="w"> </span>gcc
<span class="k">COPY</span><span class="w"> </span>hello.c<span class="w"> </span>/app/hello.c
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>
<span class="k">RUN</span><span class="w"> </span>gcc<span class="w"> </span>-o<span class="w"> </span>hello<span class="w"> </span>hello.c<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>chmod<span class="w"> </span>+x<span class="w"> </span>hello
</pre></div>
</div>
<p>stage - 2</p>
<div class="highlight-docker notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">ubuntu</span>
<span class="k">RUN</span><span class="w"> </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/usr/src/app/
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/usr/src/app</span>
<span class="k">COPY</span><span class="w"> </span>--from<span class="o">=</span>buildstep<span class="w"> </span>/app/hello<span class="w"> </span>./hello
<span class="k">COPY</span><span class="w"> </span>./start.sh<span class="w"> </span>./start.sh
<span class="k">ENV</span><span class="w"> </span><span class="nv">INITSYSTEM</span><span class="o">=</span>on
<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;bash&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;/usr/src/app/start.sh&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">COPY</span> <span class="pre">--from=buildstep</span></code> line copies only the built artifact from
the previous stage into this new stage. The C SDK and any intermediate
artifacts are not copied in the final image.</p>
</section>
</section>
<section id="packer">
<h4>Packer<a class="headerlink" href="#packer" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.packer.io/">Packer</a> is an open-source tool for creating
virtual images from a configuration file for different platforms.
Configuration files are written using <a class="reference external" href="https://github.com/hashicorp/hcl">HCL (HashiCorp Configuration
Language)</a>.</p>
<p>In general, there are three steps to create virtual images:</p>
<ul class="simple">
<li><p>Building the base image: Defined under the builders section of the
configuration file. Packer supports the following platforms: Amazon
EC2 (AMI), DigitalOcean, Docker, Google Compute Engine, Microsoft
Azure, OpenStack, Parallels (PVM), QEMU, VirtualBox (OVF), and VMware
(VMX). Administrators can add other platforms via plugins. Packer
offers an advanced feature where it supports multiple images to be
created from the same template file. It is called parallel builds,
and it ensures “near-identical” images are built for different
environments.</p></li>
<li><p>Provision of the base image for configuration: Once we build a base
image, we can then provision it to configure it further, make changes
like install software, etc. Packer supports different provisioners
such as Shell, Ansible, Puppet, and Chef.</p></li>
<li><p>Perform optional post-build operations: Post-processors allow us to
copy/move the resulted image to a central repository or create a
Vagrant box.</p></li>
</ul>
</section>
</section>
<section id="debugging-logging-and-monitoring-for-containerized-applications">
<h3>Debugging, Logging and Monitoring for Containerized Applications<a class="headerlink" href="#debugging-logging-and-monitoring-for-containerized-applications" title="Link to this heading"></a></h3>
<p>Deploying applications can be problematic and often requires analysis of
the problem. Debugging, logging and monitoring tools can help to find
the root cause of the problems. Tools such as strace, SAR (System
Activity Reporter), tcpdump, GDB (GNU Project Debugger), syslog, Nagios,
Zabbix.</p>
<p>We can use the same tools on bare metal and VMs, but containers bring
additional challenges:</p>
<ul class="simple">
<li><p>Containers are ephemeral, so when they are deleted, all their
metadata, including logs, gets deleted as well, unless we store it in
some other, possibly persistent storage location.</p></li>
<li><p>Containers do not have kernel space components.</p></li>
<li><p>We want to keep a container’s footprint as small as possible, but
installing debugging and monitoring tools make that nearly
impossible.</p></li>
<li><p>Collecting per container statistics, debugging information
individually, and then analyzing data from multiple containers is a
tedious process.</p></li>
</ul>
<p>Tools that can be used for containerized applications:</p>
<ul class="simple">
<li><p>Debugging: Docker CLI, Sysdig</p></li>
<li><p>Logging: Docker CLI, Docker Logging Driver</p></li>
<li><p>Monitoring: Docker CLI, Sysdig, cAdvisor, Prometheus, Datadog, New
Relic.</p></li>
</ul>
<section id="docker-debugging">
<h4>Docker - Debugging<a class="headerlink" href="#docker-debugging" title="Link to this heading"></a></h4>
<p>Docker has some built-in command-line options that help with debugging,
logging, and monitoring:</p>
<ul class="simple">
<li><p>Debugging:</p>
<ul>
<li><p>docker inspect</p></li>
<li><p>docker logs</p></li>
</ul>
</li>
<li><p>Logging:</p>
<ul>
<li><p>docker logs</p></li>
<li><p><a class="reference external" href="https://docs.docker.com/config/containers/logging/configure/">Docker Logging
Drivers</a>:
Logging can be performed either a Docker daemon wide or per
container logging policy. Depending on the policy, Docker forwards
the logs to the corresponding drivers. Docker supports the
following drivers: jsonfile, syslog, journald, gelf (Graylog
Extended Log Format), fluentd, awslogs, splunk. Once the logs are
saved in a central location.</p></li>
</ul>
</li>
<li><p>Monitoring:</p>
<ul>
<li><p>docker stats</p></li>
<li><p>docker top</p></li>
</ul>
</li>
</ul>
</section>
<section id="sysdig">
<h4>Sysdig<a class="headerlink" href="#sysdig" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://sysdig.com/opensource/">Sysdig</a> provides an on-cloud and
on-premise platform for container security, monitoring, and forensics.</p>
<p>It provides four open-source products:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.falco.org/">Falco</a>: Falco, the cloud-native runtime
security project, is the de facto Kubernetes threat detection engine.
Falco detects unexpected application behavior and alerts on threats
at runtime. <a class="reference external" href="https://sysdig.com/opensource/falco/">Sysdig Secure</a>
is the commercial version. Read</p>
<ul>
<li><p><a class="reference external" href="https://falco.org/docs/">Falco Project</a></p></li>
<li><p><a class="reference external" href="https://falco.org/docs/getting-started/">Falco Getting
Started</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://github.com/draios/sysdig-inspect">Sysdig Inspect</a>: Sysdig
Inspect is a powerful opensource interface for container
troubleshooting and security investigation.</p></li>
<li><p><a class="reference external" href="https://cloudcustodian.io/">Cloud Custodian</a>: Opensource Cloud
Security, Governance, and Management: Organizations can use Custodian
to manage their cloud environments, ensuring compliance to security
policies, tag policies, garbage collection of unused resources, and
cost management from a single tool.</p></li>
<li><p><a class="reference external" href="https://prometheus.io/">Prometheus</a>: Prometheus is a systems and
service monitoring system. It collects metrics from configured
targets at given intervals, evaluates rule expressions, displays the
results, and can trigger alerts when specified conditions are
observed.</p></li>
</ul>
</section>
<section id="cadvisor">
<h4>cAdvisor<a class="headerlink" href="#cadvisor" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://github.com/google/cadvisor/">cAdvisor (Container Advisor)</a>
provides container users an understanding of the resource usage and
performance characteristics of their running containers. It is a running
daemon that collects, aggregates, processes, and exports information
about running containers.</p>
<p>Read <a class="reference external" href="https://github.com/google/cadvisor/#quick-start-running-cadvisor-in-a-docker-container">Quick Start: Running cAdvisor in a Docker
Container</a>.</p>
</section>
<section id="elasticsearch">
<h4>Elasticsearch<a class="headerlink" href="#elasticsearch" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.elastic.co/elasticsearch/">Elasticsearch</a> is the
distributed search and analytics engine at the core of the Elastic
Stack. It is responsible for data indexing, search, and analysis.
<a class="reference external" href="https://www.elastic.co/kibana">Kibana</a> is an interface that enables
interactive data visualization while providing insights into the data.</p>
<p>Elasticsearch provides real-time search and analytics for structured or
unstructured text, numerical data, or geospatial data. It is optimized
to store efficiently and index data to speed up the search process.</p>
</section>
<section id="fluentd">
<h4>Fluentd<a class="headerlink" href="#fluentd" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.fluentd.org/">Fluentd</a> is an open-source data collector,
which lets you unify the data collection and consumption for better use
and understanding of data. It provides multiple
<a class="reference external" href="https://www.fluentd.org/plugins">plugins</a>.</p>
</section>
<section id="others-commercial">
<h4>Others - Commercial<a class="headerlink" href="#others-commercial" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.datadoghq.com/">Datadog</a> is the essential monitoring and
security platform for cloud applications. We bring together end-to-end
traces, metrics, and logs to make your applications, infrastructure, and
third-party services entirely observable.</p>
<p><a class="reference external" href="https://www.splunk.com/">Splunk</a> includes a family of products
aiming to deliver highly scalable and fast real-time insight into
enterprise data. It allows for data aggregation and analysis with unique
investigative methods.</p>
</section>
</section>
</section>
<section id="service-mesh">
<h2>Service Mesh<a class="headerlink" href="#service-mesh" title="Link to this heading"></a></h2>
<p>Service mesh is a network communication infrastructure layer for a
microservices-based application. When multiple microservices communicate
with each other, a service mesh allows us to decouple resilient
communication patterns such as circuit breakers and timeouts from the
application code.</p>
<section id="features-and-implementation-of-service-mesh">
<h3>Features and Implementation of Service Mesh<a class="headerlink" href="#features-and-implementation-of-service-mesh" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Service mesh is generally implemented using a sidecar proxy.</p></li>
<li><p>A sidecar is a container that runs alongside the primary application
and complements it with additional features like logging, monitoring,
and traffic routing.</p></li>
<li><p>In the service mesh architecture, the sidecar pattern implements
inter-service communication, monitoring, or other features that can
be decoupled and abstracted away from individual services.</p></li>
</ul>
<p>The following are some of the features of service mesh:</p>
<ul class="simple">
<li><p>Communication: It provides flexible, reliable, and fast communication
between various service instances.</p></li>
<li><p>Circuit Breakers: It restricts traffic to unhealthy service
instances.</p></li>
<li><p>Routing: It passes a REST request for <code class="docutils literal notranslate"><span class="pre">/foo</span></code> from the local service
instance, to which the service is connected.</p></li>
<li><p>Retries and Timeouts: It can automatically retry requests on certain
failures and can timeout requests after a specified period.</p></li>
<li><p>Service Discovery: It discovers healthy, available instances of
services.</p></li>
<li><p>Observability: It monitors latency, traces traffic flow, and
generates access logs.</p></li>
<li><p>Authentication and Authorization: It can authenticate and authorize
incoming requests.</p></li>
<li><p>Transport Layer Security (TLS) Encryption: It can secure
service-to-service communication using TLS.</p></li>
</ul>
</section>
<section id="data-plane-and-control-plane">
<h3>Data Plane and Control Plane<a class="headerlink" href="#data-plane-and-control-plane" title="Link to this heading"></a></h3>
<p>A service mesh also features Data and Control Planes.</p>
<ul class="simple">
<li><p>Service Mesh Data Plane: It provides the features mentioned above. It
touches every packet/request in the system.</p></li>
<li><p>Service Mesh Control Plane: It provides policy and configuration for
the Data Plane. For example, we can specify settings for load
balancing and circuit breakers by using the control plane.</p></li>
</ul>
<p>There are many service mesh projects, which can be divided into two
categories:</p>
<ul class="simple">
<li><p>Data Plane</p>
<ul>
<li><p>Linkerd</p></li>
<li><p>NGINX</p></li>
<li><p>HAProxy</p></li>
<li><p>Envoy</p></li>
<li><p>Traefik/Maesh.</p></li>
</ul>
</li>
<li><p>Control Plane</p>
<ul>
<li><p>Istio</p></li>
<li><p>Nelson</p></li>
<li><p>SmartStack</p></li>
</ul>
</li>
</ul>
<section id="id3">
<h4>Consul<a class="headerlink" href="#id3" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.consul.io/">Consul</a> is an open-source project aiming to
provide a secure multi-cloud service networking through automated
network configuration and service discovery.</p>
</section>
<section id="envoy">
<h4>Envoy<a class="headerlink" href="#envoy" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://www.envoyproxy.io/">Envoy</a> is an open-source edge and
service proxy designed for cloud-native applications. It provides an L7
proxy and communication bus for large, modern, service-oriented
architectures. Envoy has an out-of-process architecture, which means it
is not dependent on the application code. It runs alongside the
application and communicates with the application on localhost (as a
sidecar). With the Envoy sidecar implementation, applications need not
be aware of the network topology. Envoy can work with any language and
can be managed independently. It provides good monitoring using
statistics, logging, and distributed tracing. It can provide SSL
communication.</p>
<p>Envoy can be configured as a service and edge proxy.</p>
<ul class="simple">
<li><p>The service type of configuration is used as a communication bus for
all traffic between microservices.</p></li>
<li><p>With the edge type of configuration, it provides a single point of
ingress to the external world.</p></li>
</ul>
</section>
<section id="istio">
<h4>Istio<a class="headerlink" href="#istio" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://istio.io/">Istio</a></p>
<section id="istio-architecture">
<h5>Istio Architecture<a class="headerlink" href="#istio-architecture" title="Link to this heading"></a></h5>
<p>Istio is divided into the following two planes:</p>
<ul class="simple">
<li><p>Data Plane: It is composed of Envoy proxies deployed as sidecars to
provide a medium for communication and to control all network
communication between microservices.</p></li>
<li><p>Control Plane: It manages and configures proxies to route traffic,
enforces policies at runtime, and collects telemetry. The control
plane includes the Citadel, Gallery, and Pilot.</p></li>
</ul>
</section>
<section id="istio-components">
<h5>Istio Components<a class="headerlink" href="#istio-components" title="Link to this heading"></a></h5>
<p>The main components of Istio are:</p>
<ul class="simple">
<li><p>Envoy Proxy: Istio uses an extended version of the Envoy proxy, using
which it implements features like dynamic service discovery, load
balancing, TLS termination, circuit breakers, health checks, etc.
Envoy is deployed as sidecars.</p></li>
<li><p>Istiod: Istiod provides service discovery, configuration, and
certificate management</p></li>
</ul>
</section>
<section id="key-features-and-benefits">
<h5>Key features and benefits<a class="headerlink" href="#key-features-and-benefits" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>Traffic control to enforce fine-grained traffic control with rich
routing rules and automatic load balancing for HTTP, gRPC, WebSocket,
and TCP traffic.</p></li>
<li><p>Network resiliency to setup retries, failovers, circuit breakers, and
fault injection.</p></li>
<li><p>Security and authentication enforce security policies and enforce
access control and rate-limiting defined through the configuration
API.</p></li>
<li><p>Pluggable extensions model based on WebAssembly allows for custom
policy enforcement and telemetry generation for mesh traffic.</p></li>
</ul>
</section>
</section>
<section id="kuma">
<h4>Kuma<a class="headerlink" href="#kuma" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://kuma.io/">Kuma</a> is an open-source control plane for service
mesh, delivering security, observability, routing. It can be easily set
up on VMs, bare-metal, and Kubernetes.</p>
<p>Kuma is based on the Envoy proxy - a sidecar proxy designed for
cloud-native applications that supports monitoring, security, and
reliability for microservice applications at scale. With Envoy used as a
data plane, Kuma can handle any L4/L7 traffic to observe and route
traffic between services.</p>
<section id="kuma-architecture">
<h5>Kuma Architecture<a class="headerlink" href="#kuma-architecture" title="Link to this heading"></a></h5>
<p>Kuma includes the following two planes:</p>
<ul class="simple">
<li><p>Control-Plane: Kuma is a control plane that creates and configures
policies that manage services within the Service Mesh.</p></li>
<li><p>Data-Plane: Implemented on top of Envoy, runs as an instance together
with every service to process incoming and outgoing requests.</p></li>
</ul>
</section>
<section id="kuma-modes">
<h5>Kuma Modes<a class="headerlink" href="#kuma-modes" title="Link to this heading"></a></h5>
<p>Kuma is a universal control plane that can run on modern environments
like Kubernetes and more traditional VM instances. Such flexibility is
achieved through two separate running modes:</p>
<ul class="simple">
<li><p>Universal Mode: Installed on a Linux compatible system such as macOS,
VMs, or bare metal, including containers built on Linux-based
MicroOSes. In this mode, Kuma needs to store its state in a
PostgreSQL database.</p></li>
<li><p>Kubernetes Mode: When deployed on Kubernetes, Kuma stores its state
and configuration directly on the Kubernetes API Server, injecting a
proxy sidecar into desired Kubernetes Pods.</p></li>
</ul>
</section>
</section>
<section id="linkerd">
<h4>Linkerd<a class="headerlink" href="#linkerd" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://linkerd.io/">Linkerd</a> is an open-source network proxy. It is
ultra-light, ultra-simple, ultra-powerful. Linkerd adds security,
observability, and reliability to Kubernetes without the complexity.
Linkerd can be installed per host or instance as a replacement for the
sidecar deployment.</p>
</section>
<section id="mesh">
<h4>Mesh<a class="headerlink" href="#mesh" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://traefik.io/traefik-mesh/">Traefik Mesh</a> is a
straightforward, easy to configure, and non-invasive service mesh that
allows visibility and management of the traffic flows inside any
Kubernetes cluster. It is a simple and easy to configure service mesh
that provides traffic visibility and management inside a Kubernetes
cluster.</p>
<section id="mesh-architecture">
<h5>Mesh Architecture<a class="headerlink" href="#mesh-architecture" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p>Mesh improves cluster security through monitoring, logging,
visibility, and access controls. Communication monitoring and tracing
also help with traffic optimization and increased application
performance. Mesh can reveal underutilized resources or overloaded
services, which helps with proper resources allocation.</p></li>
<li><p>Mesh being non-invasive by design, does not require any sidecar
containers to be injected into Kubernetes pods. Instead, it routes
through proxy endpoints, called mesh controllers, that run on each
node as dedicated pods.</p></li>
</ul>
</section>
</section>
<section id="tanzu-service-mesh-commercial">
<h4>Tanzu Service Mesh - Commercial<a class="headerlink" href="#tanzu-service-mesh-commercial" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://tanzu.vmware.com/service-mesh">Tanzu Service Mesh</a> is an
enterprise-class service mesh developed by VMware, built on top of
VMware NSX. It aims to simplify the connectivity, security, and
monitoring of applications on any runtime and on any cloud. In addition,
as a modern distributed solution, it brings together application owners,
DevOps, SRE, and SecOps</p>
<p>Tanzu Service Mesh consistently connects and secures applications
running on all Kubernetes multi-clusters and multi-cloud environments.
It can be installed on VMware Tanzu Kubernetes Grid clusters or any
Kubernetes clusters, including managed Kubernetes services.</p>
</section>
</section>
</section>
<section id="internet-of-things">
<h2>Internet of Things<a class="headerlink" href="#internet-of-things" title="Link to this heading"></a></h2>
<p>The Internet of Things (IoT) is the network of physical devices,
vehicles, home appliances, and other items embedded with electronics,
software, sensors, actuators, and connectivity which enables these
things to connect and exchange data, creating opportunities for more
direct integration of the physical world into computer-based systems,
resulting in efficiency improvements, economic benefits, and reduced
human exertions.</p>
<p>Below cloud services providers that also have specialized IoT offerings</p>
<ul class="simple">
<li><p>Amazon Web Services (AWS): AWS IoT offers various products and
services like Amazon FreeRTOS, AWS IoT Core, AWS IoT Device
Management, AWS IoT Analytics, and many others.</p></li>
<li><p>Google Cloud Platform (GCP): Google Cloud IoT is the IoT offering
from Google Cloud Platform, including Cloud IoT Core and Edge TPU
services.</p></li>
<li><p>Microsoft Azure: Azure IoT offers Azure IoT Hub, Azure IoT Central,
and Azure IoT Edge, which provides artificial intelligence (AI),
Azure services, and custom logic directly on cross-platform IoT
devices.</p></li>
</ul>
</section>
<section id="serverless-computing">
<h2>Serverless Computing<a class="headerlink" href="#serverless-computing" title="Link to this heading"></a></h2>
<p>Serverless computing or just serverless is a method of running
applications without concerns about the provisioning of computer servers
or any of the compute resources. In serverless computing, we generally
write applications/functions that focus and master one particular task.
We then upload that application on the cloud provider, which gets
invoked via different events, such as HTTP requests, webhooks, etc.</p>
<p>The most common use case of serverless computing is to run any stateless
applications like data processing or real-time stream processing.
However, it can also augment a stateful application. Internet of Things
and ChatBots are common use cases of serverless computing.</p>
<p>Key features and benefits of serverless computing include:</p>
<ul class="simple">
<li><p>No Server Management: When we use serverless offerings by cloud
providers, no server management and capacity planning is required by
us. It is all handled by the cloud services provider.</p></li>
<li><p>Cost-Effective: We only need to pay for a CPU when our
applications/functions are executed. There is no charge when code is
not running. Also, there is no need to rent or purchase fixed
quantities of servers.</p></li>
<li><p>Flexible Scaling: We don’t need to set up or tune autoscaling.
Applications are automatically scaled up/down based on demand.</p></li>
<li><p>Automated High Availability and Fault Tolerance: High availability
and fault tolerance are automatically included by the underlying
cloud infrastructure providers. Developers are not required to
program for such features specifically.</p></li>
</ul>
<p>There are some drawbacks of serverless computing that include:</p>
<ul class="simple">
<li><p>Vendor Lock-In: Serverless features and implementations vary from
vendor to vendor. So, the same application/function may not behave
the same way if you change the provider, and changing your provider
can incur additional expenses.</p></li>
<li><p>Multitenancy and Security: You cannot be sure what other
applications/functions run alongside yours, which raises multitenancy
and security concerns.</p></li>
<li><p>Performance: If the application is not in use, the service provider
can take it down, which will affect performance.</p></li>
<li><p>Resource Limits: Cloud providers set resource limits for our
serverless applications/functions. Therefore, it is safer not to run
high-performance or resource-intensive workloads using serverless
solutions.</p></li>
<li><p>Monitoring and Debugging: It is more challenging to monitor
serverless applications than applications running on traditional
servers.</p></li>
</ul>
<section id="aws-lambda">
<h3>AWS Lambda<a class="headerlink" href="#aws-lambda" title="Link to this heading"></a></h3>
<p>AWS Lambda is the serverless service offered through Amazon Web Services
Compute collection of products. AWS Lambda can be triggered in different
ways, such as an HTTP request, a new document upload to S3 bucket, a
scheduled job, and AWS Kinesis data stream, a notification from AWS
Simple Notification Service, or a REST API call through the Amazon API
Gateway.</p>
</section>
<section id="google-cloud-functions">
<h3>Google Cloud Functions<a class="headerlink" href="#google-cloud-functions" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://cloud.google.com/functions/">Google Cloud Functions</a> is
essentially a Function-as-a-Service (FaaS) offering and it is
complemented by two additional services, App Engine and Cloud Run:</p>
<ul class="simple">
<li><p>App Engine allows users to build highly scalable applications on a
fully managed serverless platform. It supports Node.js, Java, Ruby,
C#, Go, Python, PHP, or other programming languages through custom
runtime support. Scalability and flexibility are achieved by
encapsulating the applications in Docker containers.</p></li>
<li><p>Cloud Run is a managed compute platform for fast and secure
deployment and scaling of containerized applications. It supports
Node.js, Java, Ruby, Go, and Python, and it improves the developer
experience by integrating with services such as Cloud Code, Cloud
Build, Cloud Monitoring, and Cloud Logging. Cloud Run also enables
application portability by implementing the Knative open standard and
supporting the Docker runtime.</p></li>
</ul>
<p>Google Cloud Functions can be written in Node.js, Python, Go, or Java
and can be executed on Debian or Ubuntu systems on the Google Cloud
Platform, simplifying and easing portability and local testing.</p>
</section>
<section id="azure-functions">
<h3>Azure Functions<a class="headerlink" href="#azure-functions" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://azure.microsoft.com/en-us/services/functions/">Azure
Functions</a> is
part of Microsoft Azure’s serverless compute offering, together with
Serverless Kubernetes and Serverless application environments. Azure
Functions offers an event-driven environment. It is available as a
managed service in Azure and Azure Stack, but it also works on
Kubernetes, Azure IoT Edge, on-premises, and other clouds.</p>
<ul class="simple">
<li><p>Serverless Kubernetes allows users to create serverless,
Kubernetes-based applications orchestrated with Azure Kubernetes
Service (AKS) and AKS virtual nodes, based on the open-source Virtual
Kubelet project.</p></li>
<li><p>Serverless application environments allow the running and scaling of
web, mobile, and API applications on any platform of your choice
through Azure App Service.</p></li>
</ul>
</section>
<section id="serverless-computing-1">
<span id="id4"></span><h3>Serverless Computing<a class="headerlink" href="#serverless-computing-1" title="Link to this heading"></a></h3>
<section id="projects-that-use-containers-to-execute-serverless-applications">
<h4>Projects That Use Containers to Execute Serverless Applications<a class="headerlink" href="#projects-that-use-containers-to-execute-serverless-applications" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://azure.microsoft.com/en-in/services/container-instances/">Azure Container
Instances</a>
: Azure Container Instances (ACI) is the service offered by Microsoft
Azure, which allows users to run containers without managing servers.
It provides hypervisor isolation for each container group to ensure
containers run in isolation without sharing a kernel.</p></li>
<li><p><a class="reference external" href="https://aws.amazon.com/fargate/">AWS Fargate</a>: AWS Fargate is the
service offered by Amazon Web Services, providing serverless compute
for containers. It runs containers on Amazon Elastic Container
Service (ECS) and Amazon Elastic Kubernetes Service (EKS) services.</p></li>
<li><p><a class="reference external" href="https://fission.io/">Fission</a>: Fission is an open-source project
from Platform9, which provides a serverless Function-as-a-Service
(FaaS) framework on Kubernetes.</p></li>
<li><p><a class="reference external" href="https://fnproject.io/">Fn Project</a>: Fn Project is an open-source
container-native serverless platform that runs on any platform, local
or cloud.</p></li>
<li><p><a class="reference external" href="https://virtual-kubelet.io/">Virtual Kubelet</a>: Virtual Kubelet
connects Kubernetes to other APIs and masquerades them as Kubernetes
nodes. These other APIs include ACI, Fargate, and IoT Edge.</p></li>
<li><p><a class="reference external" href="https://knative.dev/">Knative</a>: Kubernetes-based platform to
deploy and manage modern serverless workloads. Knative Features:</p>
<ul>
<li><p>Serving: Run serverless containers on Kubernetes with ease.
Knative takes care of the details of networking, autoscaling (even
to zero), and revision tracking. You just have to focus on your
core logic.</p></li>
<li><p>Eventing: Universal subscription, delivery, and management of
events. Build modern apps by attaching compute to a data stream
with declarative event connectivity and a developer-friendly
object model.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.openfaas.com/">OpenFaaS</a>: is an open-source project
that aims to simplify functions and code deployment directly to
Kubernetes, OpenShift, or Docker Swarm.</p></li>
</ul>
</section>
</section>
</section>
<section id="distributed-tracing">
<h2>Distributed Tracing<a class="headerlink" href="#distributed-tracing" title="Link to this heading"></a></h2>
<p>Organizations are adopting microservices for agility, easy deployment,
scaling, etc. However, with many services working together, sometimes it
becomes difficult to pinpoint the root cause of latency issues or
unexpected behaviors. To overcome such situations, we would need to
instrument the behavior of each participating service in our
microservices-based application. After collecting and combining the
instrumented data from each participating service, we should gain
visibility of the entire system. This is generally referred to as
distributed tracing.</p>
<p><a class="reference external" href="https://opentelemetry.io/">OpenTelemetry</a>: An observability
framework for cloud-native software. OpenTelemetry is a collection of
tools, APIs, and SDKs to the instrument, generate, collect, and export
telemetry data (metrics, logs, and traces) for analysis to understand
the software’s performance and behavior.</p>
<p>In general, a trace represents the details about a transaction, like how
much time it took to call a specific function. In OpenTracing, a trace
is referred to by the directed acyclic graph (DAG) of spans. Each span
can be referred to as a timed operation between contiguous segments of
work.</p>
<p>Using OpenTracing, we collect tracing spans for our services and then
forward them to different tracers. Tracers are then used to monitor and
troubleshoot microservices-based applications. Supported tracers for
OpenTracing include <a class="reference external" href="https://jaegertracing.io/">Jaeger</a>,
<a class="reference external" href="https://lightstep.com/">LightStep</a>,
<a class="reference external" href="https://www.instana.com/">Instana</a>, <a class="reference external" href="https://www.elastic.co/apm">Elastic
APM</a>,
<a class="reference external" href="https://wavefront.com/">Wavefront</a>.</p>
</section>
<section id="hosting-providers-github-and-more">
<h2>Hosting Providers: GitHub and More<a class="headerlink" href="#hosting-providers-github-and-more" title="Link to this heading"></a></h2>
<p>There are other sites that offer similar services, including:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://about.gitlab.com/">GitLab</a></p></li>
<li><p><a class="reference external" href="https://www.gitkraken.com/">GitKraken</a></p></li>
<li><p><a class="reference external" href="https://launchpad.net/">Launchpad</a>.</p></li>
</ul>
<section id="github-repositories-types">
<h3>GitHub Repositories - Types<a class="headerlink" href="#github-repositories-types" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Private Repositories: Only selected collaborators can see the
repository, or clone it (make a local copy), or download its contents
in a variety of forms. The owner must specifically authorize each
collaborator.</p></li>
<li><p>Public Repositories: Anyone given the proper link, such as
<code class="docutils literal notranslate"><span class="pre">https:/github.com/an-account/a-repo</span></code>, can copy, clone or fork the
repository, or download its contents. However, unless the owner
authorizes them as a collaborator, one does not have permission to
upload or make modifications.</p></li>
</ul>
</section>
<section id="advanced-git-interfaces-gerrit">
<h3>Advanced Git Interfaces: Gerrit<a class="headerlink" href="#advanced-git-interfaces-gerrit" title="Link to this heading"></a></h3>
<p>Git has well-established methods of workflow. It is pretty flexible and
projects can choose quite different approaches, but it is always based
on a cycle of:</p>
<ul class="simple">
<li><p>Make changes in the code, probably in a development branch.</p></li>
<li><p>Commit those changes to the branch; there may be one or more changes
(patches) per commit.</p></li>
<li><p>Publish those changes through a push or a pull request.</p></li>
<li><p>The changes will be reviewed and merged if necessary or sent back for
further work.</p></li>
</ul>
<p>Gerrit is built completely on Git, but it adds another layer of code
review before changes are committed to the authoritative master
repository. This review is likely to be done by multiple contributors
rather than just one powerful maintainer. While such a workflow is not
exactly new (most projects have multiple reviewers with some structure
for who makes the ultimate decisions) the Gerrit architecture is
designed to formalize this procedure.</p>
<p>Gerrit introduces a reviewing layer that lies between the contributors
and the upstream repository.</p>
<ul class="simple">
<li><p>Contributors submit their work (one change per submission is best) to
the reviewing layer.</p></li>
<li><p>Contributors pull the latest upstream changes from the upstream
layer.</p></li>
<li><p>Reviewers one the ones who submit work to the upstream layer.</p></li>
<li><p>The reviewers evaluate pending changes and discuss them. According to
project governing procedures they can grant approval and submit
upstream, or they can reject or request modifications.</p></li>
</ul>
<p>Gerrit also records comments about each pending request and preserve
them in a record which can be consulted at any time to provide
documentation about why and how modifications were made.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="LFF-ESS-P0B-LinuxEssentials.html" class="btn btn-neutral float-left" title="Linux Basics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="LFF-ESS-P0E-OpenSource.html" class="btn btn-neutral float-right" title="Open Source Concepts" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Vijay Kumar &amp; Contributors.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script> 

</body>
</html>